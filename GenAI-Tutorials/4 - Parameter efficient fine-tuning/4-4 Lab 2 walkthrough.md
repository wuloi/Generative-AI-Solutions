### ğŸ§  **ç”¨LoRAå¯¹Flan-T5è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼šä¸€åœºPEFTä¹‹æ—…**

å¤§å®¶å¥½ï¼ŒæŠ€æœ¯çˆ±å¥½è€…ä»¬ï¼ğŸ‘‹ æœ¬å‘¨åœ¨æˆ‘ä»¬çš„å®éªŒå®¤é‡Œï¼Œæˆ‘ä»¬ä¸ä»…ä»…æ˜¯åœ¨ç©ç«ï¼Œæˆ‘ä»¬è¿˜åœ¨å¾®è°ƒå®ƒï¼ğŸ”¥ åŠ å…¥æˆ‘ä»¬ï¼Œä¸€èµ·è¸ä¸Šä½¿ç”¨LoRAè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ¥å¢å¼ºFlan-T5æ¨¡å‹æ‘˜è¦èƒ½åŠ›çš„æ—…ç¨‹ã€‚ç³»å¥½å®‰å…¨å¸¦ï¼Œæˆ‘çš„åŒäº‹Chriså°†å¸¦é¢†æˆ‘ä»¬ç©¿è¶Šæœ¬å‘¨ç¼–ç éš¾é¢˜çš„æ›²æŠ˜ã€‚

#### **å®éªŒå®¤2ï¼šåŠ¨æ‰‹å†’é™©**

åœ¨å®éªŒå®¤2ä¸­ï¼Œæˆ‘ä»¬ä»é›¶æ ·æœ¬æ¨ç†å‡çº§åˆ°**å®Œå…¨å¾®è°ƒ**ã€‚æˆ‘ä»¬ç”¨æˆ‘ä»¬è‡ªå·±çš„æç¤ºå®šåˆ¶Flan-T5æ¨¡å‹ï¼Œè¿›è¡Œä¸€ä¸ªåƒé›ªèŠ±ä¸€æ ·ç‹¬ç‰¹çš„æ‘˜è¦ä»»åŠ¡ã€‚è®©æˆ‘ä»¬æ·±å…¥ç¬”è®°æœ¬ï¼Œçœ‹çœ‹æˆ‘ä»¬èƒ½æ–½å±•å‡ºä»€ä¹ˆé­”æ³•ã€‚

#### **åœ¨AWSå’ŒPyTorchä¸Šæ­å»ºèˆå°**

å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ æœ‰åˆé€‚çš„ç¡¬ä»¶ï¼šä¸€ä¸ªAWS SageMakerå®ä¾‹ç±»å‹ml.m5.2xlï¼Œé…å¤‡å…«ä¸ªCPUå’Œ32GBçš„RAMã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†PyTorchå’Œtorchdataåº“ï¼Œä»¥å®ç°æ— ç¼çš„æ•°æ®åŠ è½½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰ç”¨äºè®¡ç®—ROUGEå¾—åˆ†çš„è¯„ä¼°å·¥å…·ï¼Œè¿™æ˜¯è¡¡é‡æ‘˜è¦è´¨é‡çš„é»„é‡‘æ ‡å‡†ã€‚

#### **LoRAå’ŒPEFTï¼šæƒé‡ä¿®æ”¹çš„å·«å¸ˆ**

æˆ‘ä»¬æœ‰ä¸¤ä¸ªæ–°çš„åº“ï¼šLoRAå’ŒPEFTã€‚è¿™äº›æ˜¯æˆ‘ä»¬å¾®è°ƒçš„ç§˜å¯†æ­¦å™¨ï¼Œè€Œä¸éœ€è¦å…¨é¢ä¿®æ”¹æ¨¡å‹çš„å‚æ•°ã€‚è¿™å°±åƒæ˜¯æ‹¥æœ‰ä¸€æ ¹åªè§¦åŠä¸€å°éƒ¨åˆ†é­”æ³•ä¹¦çš„é­”æ–ã€‚

#### **TrainingArgumentså’ŒTrainerï¼šå˜å½¢è€…çš„åŠ©æ‰‹**

æˆ‘ä»¬ä»transformersåº“ä¸­å¯¼å…¥TrainingArgumentså’ŒTrainerã€‚è¿™äº›ç±»æ˜¯æˆ‘ä»¬å¯é çš„åŠ©æ‰‹ï¼Œç®€åŒ–äº†ä»£ç ï¼Œç¡®ä¿æˆ‘ä»¬çš„æ¨¡å‹è®­ç»ƒåƒé»„æ²¹ä¸€æ ·é¡ºæ»‘ã€‚

#### **å®Œå…¨å¾®è°ƒï¼šé‡é‡çº§è®­ç»ƒ**

æˆ‘ä»¬ä»å®Œå…¨å¾®è°ƒå¼€å§‹ï¼Œä¸ºæˆ‘ä»¬ç‰¹å®šçš„æ•°æ®é›†ä¿®æ”¹è¯­è¨€æ¨¡å‹çš„æƒé‡ã€‚è¿™å°±åƒæ˜¯ç»™æˆ‘ä»¬çš„æ¨¡å‹ä¸€ä¸ªä¸ªæ€§åŒ–çš„è®­ç»ƒï¼Œè®©å®ƒä¸ºæ‘˜è¦ä»»åŠ¡åšå¥½å¤æ—¥å‡†å¤‡ã€‚

#### **ROUGEï¼šæ‘˜è¦çš„è£åˆ¤**

ä½¿ç”¨ROUGEï¼Œæˆ‘ä»¬å°†è¯„ä¼°æˆ‘ä»¬çš„æ‘˜è¦å¦‚ä½•æ•æ‰åŸæ–‡çš„æœ¬è´¨ã€‚è¿™æ˜¯è¯­è¨€æµ‹è¯•çš„è¯•é‡‘çŸ³ï¼Œå‘Šè¯‰æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦å‘½ä¸­äº†ç›®æ ‡ã€‚

#### **PEFTåœ¨è¡ŒåŠ¨ï¼šæ›´è½»æŸ”çš„è§¦æ‘¸**

ç°åœ¨ï¼Œè®©æˆ‘ä»¬è½¬å‘ä½¿ç”¨LoRAçš„PEFTã€‚æˆ‘ä»¬åªè®­ç»ƒäº†æ¨¡å‹å‚æ•°çš„1.4%ã€‚è¿™å°±åƒæ˜¯ä¸ºé©¬æ‹‰æ¾è¿›è¡Œçš„é’ˆå¯¹æ€§è®­ç»ƒâ€”â€”ä¸“æ³¨è€Œé«˜æ•ˆã€‚

#### **æ¯”è¾ƒç­–ç•¥ï¼šROUGEå¤§æˆ˜**

æˆ‘ä»¬å°†æŠŠæˆ‘ä»¬å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œä¸åŸå§‹çš„Flan-T5è¿›è¡Œæ¯”è¾ƒã€‚ROUGEæŒ‡æ ‡å°†æ˜¯æˆ‘ä»¬è£åˆ¤ï¼Œç»“æœå°†å¤§æœ‰è£¨ç›Šã€‚

#### **å®šæ€§æ´å¯Ÿï¼šäººç±»è§†è§’ä¸‹çš„æ¨¡å‹**

æˆ‘ä»¬å°†ä»å®šæ€§çš„è§’åº¦æŸ¥çœ‹æ¨¡å‹çš„è¾“å‡ºï¼Œå°†å®ƒä»¬å¹¶æ’æ¯”è¾ƒã€‚è¿™å°±åƒæ˜¯ä¸€ä¸ªå“å°æµ‹è¯•ï¼Œä½†é’ˆå¯¹çš„æ˜¯AIç”Ÿæˆçš„æ‘˜è¦ã€‚

#### **å®šé‡éªŒè¯ï¼šROUGEæŒ‡æ ‡**

æˆ‘ä»¬å°†è¿è¡Œæ•°å­—ï¼Œæ¯”è¾ƒåŸå§‹ã€æŒ‡ä»¤å¾®è°ƒå’ŒPEFTå¾®è°ƒæ¨¡å‹çš„ROUGEå¾—åˆ†ã€‚æ•°æ®ä¸ä¼šæ’’è°ï¼Œå®ƒå°†æ­ç¤ºå“ªç§æ–¹æ³•çœŸæ­£æ˜¯å† å†›ã€‚

#### **æ€»ç»“ï¼šPEFTçš„ä¼˜åŠ¿**

æœ€åï¼Œæˆ‘ä»¬å°†çœ‹åˆ°PEFTåœ¨è®¡ç®—èµ„æºå’Œæ•ˆç‡æ–¹é¢æä¾›äº†æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚è¿™æ˜¯ä¸€ç§æ™ºèƒ½çš„å¾®è°ƒæ–¹å¼ï¼Œç‰¹åˆ«æ˜¯å½“ä½ çš„èµ„æºæœ‰é™æ—¶ã€‚

---

ä¸è¦é”™è¿‡è¿™æ¬¡æ¿€åŠ¨äººå¿ƒçš„PEFTå’ŒLoRAå¾®è°ƒæ¢ç´¢ï¼è®¢é˜…æˆ‘ä»¬çš„é¢‘é“ï¼Œè·å–æ›´å¤šæ·±å…¥çš„æŠ€æœ¯å†’é™©ï¼Œè®©æˆ‘ä»¬ç»§ç»­æ¨åŠ¨AIçš„å¯èƒ½æ€§è¾¹ç•Œã€‚ä¸‹æ¬¡è§ï¼Œç»§ç»­ç¼–ç ï¼Œç»§ç»­å­¦ä¹ ï¼Œæ„¿ä½ çš„æ¨¡å‹æ°¸è¿œç²¾å‡†è°ƒä¼˜ï¼ğŸŒŸ

[ç‚¹å‡»æŸ¥çœ‹å®Œæ•´å®éªŒå®¤](https://www.youtube.com/watch?v=dQw4w9WgXcQ)

---

[åŠ å…¥æˆ‘ä»¬ï¼Œç»§ç»­æˆ‘ä»¬åœ¨AIå®‡å®™ä¸­çš„æ—…ç¨‹ï¼](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---

### ğŸ§  **Fine-Tuning Flan-T5 with LoRA: A PEFT Odyssey**

Hey tech enthusiasts! ğŸ‘‹ This week on our lab bench, we're not just playing with fire, we're fine-tuning it! ğŸ”¥ Join us as we embark on a journey to enhance the summarization prowess of the Flan-T5 model using Parameter-Efficient Fine-Tuning (PEFT) with LoRA. Strap in as my colleague Chris guides us through the twists and turns of this week's coding conundrum.

#### **Lab 2: The Hands-On Adventure**

In Lab 2, we're stepping up our game from zero-shot inference to **full fine-tuning**. We're customizing the Flan-T5 model with our own prompts for a summarization task that's as unique as a snowflake. Let's dive into the notebook and see what magic we can conjure up.

#### **Setting the Stage with AWS and PyTorch**

Before we start, make sure you've got the right hardware: an AWS SageMaker instance type ml.m5.2xl with eight CPUs and 32GB of RAM. We're also piping in PyTorch and the torchdata library for seamless data loading. Plus, we've got evaluates for calculating ROUGE scores, the gold standard for measuring summary quality.

#### **LoRA and PEFT: The Wizards of Weight Modification**

We've got two new libraries in town: LoRA and PEFT. These are our secret weapons for fine-tuning without going full hog on the model's parameters. It's like having a magic wand that only touches a fraction of the spellbook.

#### **TrainingArguments and Trainer: The Transformers' Helpers**

From the transformers library, we're importing TrainingArguments and Trainer. These classes are our trusty sidekicks, simplifying the code and ensuring our model training is as smooth as butter.

#### **Full Fine-Tuning: The Heavy Lifting**

We're starting with full fine-tuning, where we modify the weights of our language model for our specific dataset. It's like giving our model a personalized workout to get it summer-ready for summarization tasks.

#### **ROUGE: The Judge of Summaries**

Using ROUGE, we'll evaluate how well our summaries encapsulate the essence of the original text. It's the linguistic litmus test that tells us if our model is hitting the mark.

#### **PEFT in Action: The Lighter Touch**

Now, let's shift gears to PEFT with LoRA. We're training a mere 1.4% of the model's parameters. It's like spot training for a marathonâ€”focused and efficient.

#### **Comparing Strategies: The ROUGE Rumble**

We'll put our fine-tuned models to the test, comparing them to the original Flan-T5. The ROUGE metrics will be our judge, and the results will speak volumes.

#### **Qualitative Insights: A Human Eye on the Models**

We'll take a qualitative look at the models' outputs, comparing them side by side. It's like a taste test, but for AI-generated summaries.

#### **Quantitative Validation: The ROUGE Metrics**

We'll run the numbers, comparing the ROUGE scores of the original, instruction fine-tuned, and PEFT fine-tuned models. The data doesn't lie, and it will reveal which approach is truly the champion.

#### **Wrapping Up: The PEFT Advantage**

In the end, we'll see that PEFT offers a significant advantage in terms of compute resources and efficiency. It's the smart way to fine-tune, especially when you're working with limited resources.

---

Don't miss out on this thrilling exploration of fine-tuning with PEFT and LoRA! Subscribe to our channel for more in-depth tech adventures, and let's keep pushing the boundaries of what's possible with AI. Until next time, keep coding, keep learning, and may your models always be finely tuned! ğŸŒŸ

[Check out the full lab here](https://www.youtube.com/watch?v=dQw4w9WgXcQ)

---

[Join us for the next episode on our journey through the AI universe!](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---

### ç§‘æ™®æŠ€æœ¯æ–‡ç« ï¼šæå‡æ–‡æœ¬æ‘˜è¦èƒ½åŠ›çš„é«˜æ•ˆæ–¹æ³•

#### å¼•è¨€
åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸï¼Œæ–‡æœ¬æ‘˜è¦æ˜¯ä¸€é¡¹å…³é”®æŠ€æœ¯ï¼Œå®ƒèƒ½å¤Ÿå°†é•¿æ–‡æœ¬å‹ç¼©æˆç®€æ´çš„æ‘˜è¦ï¼ŒåŒæ—¶ä¿ç•™åŸæ–‡çš„æ ¸å¿ƒä¿¡æ¯ã€‚éšç€æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯åŸºäºTransformeræ¶æ„çš„æ¨¡å‹ï¼Œæ–‡æœ¬æ‘˜è¦çš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚æœ¬æ–‡å°†ä»‹ç»ä¸€ç§åä¸ºLoRAçš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯ï¼Œå®ƒé€šè¿‡å¾®è°ƒå°‘é‡å‚æ•°æ¥æé«˜Flan-T5æ¨¡å‹çš„æ–‡æœ¬æ‘˜è¦èƒ½åŠ›ã€‚

#### å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ä¸LoRA
PEFTæ˜¯ä¸€ç§å¾®è°ƒæ–¹æ³•ï¼Œå®ƒé€šè¿‡è°ƒæ•´æ¨¡å‹ä¸­ä¸€å°éƒ¨åˆ†å‚æ•°æ¥é€‚åº”ç‰¹å®šçš„ä»»åŠ¡ï¼Œè€Œä¸æ˜¯è°ƒæ•´æ•´ä¸ªæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•çš„ä¼˜åŠ¿åœ¨äºå‡å°‘äº†è®¡ç®—èµ„æºçš„éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„æ€§èƒ½ã€‚LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯PEFTçš„ä¸€ç§å®ç°æ–¹å¼ï¼Œå®ƒé€šè¿‡å¼•å…¥ä½ç§©çŸ©é˜µæ¥å¾®è°ƒæ¨¡å‹çš„æƒé‡ã€‚

#### å®éªŒç¯å¢ƒä¸å·¥å…·
åœ¨æœ¬å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†AWSçš„SageMakeræœåŠ¡ï¼Œå®ä¾‹ç±»å‹ä¸ºml.m5.2xlï¼Œé…å¤‡äº†8ä¸ªCPUå’Œ32GBå†…å­˜ã€‚å®éªŒæ¶‰åŠåˆ°çš„åº“åŒ…æ‹¬PyTorchã€torchdataã€evaluatesç­‰ï¼Œè¿™äº›åº“å¸®åŠ©æˆ‘ä»¬è¿›è¡Œæ•°æ®åŠ è½½ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚

#### å®éªŒæ­¥éª¤
1. **æ•°æ®åŠ è½½ä¸æ¨¡å‹åˆå§‹åŒ–**ï¼šåŠ è½½è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶åˆå§‹åŒ–Flan-T5æ¨¡å‹å’Œtokenizerã€‚
2. **å…¨å‚æ•°å¾®è°ƒ**ï¼šé¦–å…ˆè¿›è¡Œå…¨å‚æ•°å¾®è°ƒï¼Œå³è°ƒæ•´æ¨¡å‹çš„æ‰€æœ‰å‚æ•°ä»¥é€‚åº”æ‘˜è¦ä»»åŠ¡ã€‚
3. **å‚æ•°é«˜æ•ˆå¾®è°ƒ**ï¼šç„¶åä½¿ç”¨LoRAæŠ€æœ¯è¿›è¡ŒPEFTï¼Œåªè°ƒæ•´æ¨¡å‹ä¸­ä¸€å°éƒ¨åˆ†å‚æ•°ã€‚

#### å®éªŒç»“æœä¸åˆ†æ
- **å…¨å‚æ•°å¾®è°ƒ**ï¼šé€šè¿‡å…¨å‚æ•°å¾®è°ƒï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ¨¡å‹åœ¨ROUGEè¯„ä¼°æŒ‡æ ‡ä¸Šæœ‰äº†æ˜¾è‘—æå‡ï¼Œä¸åŸå§‹Flan-T5æ¨¡å‹ç›¸æ¯”ï¼Œæ‘˜è¦çš„è´¨é‡å¾—åˆ°äº†æ”¹å–„ã€‚
- **å‚æ•°é«˜æ•ˆå¾®è°ƒ**ï¼šå°½ç®¡PEFTæ¨¡å‹åœ¨ROUGEæŒ‡æ ‡ä¸Šç•¥æœ‰ä¸‹é™ï¼Œä½†èµ„æºæ¶ˆè€—å¤§å¤§å‡å°‘ï¼Œä½¿å¾—åœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¿›è¡Œæœ‰æ•ˆçš„æ¨¡å‹å¾®è°ƒã€‚

#### è®¨è®º
PEFTå’ŒLoRAæŠ€æœ¯å±•ç¤ºäº†åœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆæ¨¡å‹å¾®è°ƒçš„å¯èƒ½æ€§ã€‚é€šè¿‡åªè°ƒæ•´æ¨¡å‹çš„ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å‡å°‘è®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚è¿™å¯¹äºå¤§è§„æ¨¡éƒ¨ç½²å’Œå®æ—¶åº”ç”¨åœºæ™¯å°¤ä¸ºé‡è¦ã€‚

#### ç»“è®º
æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨PEFTå’ŒLoRAæŠ€æœ¯æ¥æå‡Flan-T5æ¨¡å‹çš„æ–‡æœ¬æ‘˜è¦èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹ï¼ŒPEFTä¹Ÿèƒ½æä¾›ä¸€ç§æœ‰æ•ˆçš„æ¨¡å‹å¾®è°ƒæ–¹æ³•ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚éšç€NLPæŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œæˆ‘ä»¬æœŸå¾…çœ‹åˆ°æ›´å¤šåˆ›æ–°çš„æ–¹æ³•æ¥è§£å†³å®é™…é—®é¢˜ã€‚

---

**æ³¨**ï¼šæœ¬æ–‡ä¸ºç§‘æ™®æ€§è´¨çš„æŠ€æœ¯æ–‡ç« ï¼Œæ—¨åœ¨å‘éä¸“ä¸šè¯»è€…ä»‹ç»PEFTä¸LoRAæŠ€æœ¯åœ¨æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚

---

[åŠ å…¥æˆ‘ä»¬ï¼Œç»§ç»­æˆ‘ä»¬åœ¨AIå®‡å®™ä¸­çš„æ—…ç¨‹ï¼](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---
