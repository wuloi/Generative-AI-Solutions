# ğŸ¬ æŒæ§LLMsï¼šä¸ºä¸‹ä¸€è¯é­”æ³•æ‰“é€ å®Œç¾æç¤º

å˜¿ï¼ŒæŠ€æœ¯å¼€æ‹“è€…ä»¬ï¼ğŸŒŸ å‡†å¤‡å¥½æå‡ä½ çš„AIæŠ€èƒ½ï¼Œæˆ‘ä»¬å°†æ¢ç´¢å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆéå‡¡æ–‡æœ¬çš„è‰ºæœ¯ã€‚è®©æˆ‘ä»¬æ·±å…¥äº†è§£å¦‚ä½•é…ç½®è¿™äº›æ¨¡å‹ï¼Œè®©å®ƒä»¬æŒ‰ç…§ä½ çš„æ„æ„¿æ–½å±•é­”åŠ›ï¼

## ğŸ”§ ç²¾é€šæ¨¡å‹é…ç½®ï¼šLLMæ€§èƒ½çš„å…³é”®

ä½ æ˜¯å¦æ›¾åœ¨Hugging Faceæˆ–AWSç­‰å¹³å°ä¸Šç©è¿‡LLMsï¼Ÿä½ å¯èƒ½æ³¨æ„åˆ°äº†é‚£äº›å¯ä»¥è®©ä½ è°ƒæ•´æ¨¡å‹è¡Œä¸ºçš„å·§å¦™æ§åˆ¶ã€‚è¿™äº›ä¸æ˜¯ä½ å¸¸è§„çš„è®­ç»ƒå‚æ•°ï¼›å®ƒä»¬æ˜¯æ¨ç†æ—¶çš„é…ç½®ï¼Œè®©ä½ æŒæ¡æ§åˆ¶æƒã€‚

### ğŸš« æœ€å¤§æ–°è¯æ•°ï¼šè¯ç”Ÿæˆå™¨çš„ä¸Šé™
å°†**æœ€å¤§æ–°è¯æ•°**æƒ³è±¡æˆé™åˆ¶æ¨¡å‹é€‰æ‹©è¯çš„æ¬¡æ•°ã€‚å°±åƒæ˜¯å‘Šè¯‰æ¨¡å‹ï¼šâ€œå˜¿ï¼Œé€‰äº†è¿™ä¹ˆå¤šè¯ä¹‹åï¼Œå°±å·®ä¸å¤šè¯¥ç»“æŸäº†ï¼â€

### ğŸŒ¡ï¸ æ¸©åº¦ï¼šéšæœºæ€§çš„è°ƒèŠ‚å™¨
è°ƒæ•´**æ¸©åº¦**æ¥æ§åˆ¶éšæœºæ€§ã€‚æ›´é«˜çš„æ¸©åº¦æ„å‘³ç€æ›´æœ‰åˆ›é€ æ€§ï¼Œä½†å¯èƒ½æ›´ç‹‚é‡çš„è¾“å‡ºã€‚é™ä½å®ƒä»¥è·å¾—æ›´å¯é¢„æµ‹çš„æ–‡æœ¬ï¼ŒåšæŒæ¨¡å‹æœ€æ“…é•¿çš„å†…å®¹ã€‚

## ğŸ° ä¸‹ä¸€è¯é¢„æµ‹ï¼šè¶…è¶Šè´ªå©ªè§£ç 

é»˜è®¤æƒ…å†µä¸‹ï¼ŒLLMsä½¿ç”¨**è´ªå©ªè§£ç **ï¼Œé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯ã€‚ä½†è¿™å¯èƒ½ä¼šå˜å¾—é‡å¤ã€‚æ˜¯æ—¶å€™æ··åˆä¸€ä¸‹äº†ï¼

### ğŸ° éšæœºé‡‡æ ·ï¼šå¼•å…¥å¯å˜æ€§
ä½¿ç”¨**éšæœºé‡‡æ ·**ï¼Œæ¨¡å‹æ ¹æ®å…¶æ¦‚ç‡åˆ†æ•°é€‰æ‹©è¯ï¼Œå‡å°‘é‡å¤å¹¶ä¸ºæ··åˆæ·»åŠ ä¸€ä¸ä¸å¯é¢„æµ‹æ€§ã€‚

### ğŸ” Top Ké‡‡æ ·ï¼šç²¾è‹±ä¸­çš„ç²¾è‹±
é™åˆ¶æ¨¡å‹åªé€‰æ‹©æœ€å¯èƒ½çš„**k**ä¸ªè¯ã€‚å°±åƒæ˜¯å‘Šè¯‰æ¨¡å‹ï¼šâ€œåªä»æœ€å¥½çš„ä¸­æŒ‘é€‰ï¼â€

### ğŸ¯ Top Pé‡‡æ ·ï¼šæ¦‚ç‡æ± 
æˆ–è€…ï¼Œä½¿ç”¨**top p**åªè€ƒè™‘æ€»å’Œè¾¾åˆ°æŸä¸ªæ¦‚ç‡çš„é¢„æµ‹ã€‚å°±åƒæ˜¯è¯´ï¼šâ€œè®©æˆ‘ä»¬ä¿æŒåˆç†ï¼Œå¥½å—ï¼Ÿâ€

## ğŸ› ï¸ å°è¯•æ¨ç†å‚æ•°ï¼šæ–‡æœ¬ç”Ÿæˆçš„å®éªŒå®¤

ç©è¿™äº›å‚æ•°å°±åƒåœ¨è¯­è¨€å¨æˆ¿é‡Œå½“å¨å¸ˆâ€”â€”æ··åˆæ­é…ï¼Œåˆ›é€ å‡ºå®Œç¾çš„æ–‡æœ¬å¤§é¤ã€‚ä½†è®°ä½ï¼Œä¸Šä¸‹æ–‡çª—å£æœ‰é™åˆ¶ï¼Œæ‰€ä»¥æ¯ä¸ªè¯éƒ½è¦ç®—æ•°ï¼

## ğŸ“ˆ æ‰©å±•æ¨¡å‹çš„ç†è§£ï¼šå¤§å°å¾ˆé‡è¦

æ¨¡å‹è¶Šå¤§ï¼Œåœ¨é›¶æ ·æœ¬æ¨ç†ä¸­è¡¨ç°å¾—è¶Šå¥½ï¼Œè½»æ¾å¤„ç†å®ƒæ²¡æœ‰æ˜ç¡®è®­ç»ƒè¿‡çš„ä»»åŠ¡ã€‚è¾ƒå°çš„æ¨¡å‹ï¼Ÿå®ƒä»¬å¯èƒ½éœ€è¦æ›´å¤šçš„æŒ‡å¯¼ã€‚

## ğŸ”® æ€»ç»“ï¼šç”¨é…ç½®è¶…èƒ½åŠ›è£…å¤‡è‡ªå·±

ä½ å·²ç»æŒæ¡äº†çŸ¥è¯†ï¼Œç°åœ¨æ˜¯åº”ç”¨å®ƒçš„æ—¶å€™äº†ã€‚æ— è®ºä½ æ˜¯åœ¨å¾®è°ƒè¿˜æ˜¯åœ¨å°è¯•æç¤ºï¼Œä½ éƒ½åœ¨å­¦ä¹ æŒæ¡LLMsçš„é“è·¯ä¸Šã€‚åœ¨ä¸‹ä¸€ä¸ªè§†é¢‘ä¸­ï¼Œæˆ‘ä»¬å°†æ›´è¿›ä¸€æ­¥ï¼Œæ¢ç´¢å¦‚ä½•å¼€å‘å’Œå¯åŠ¨ä¸€ä¸ªç”±LLMé©±åŠ¨çš„åº”ç”¨ç¨‹åºã€‚

åˆ«å¿˜äº†ç‚¹å‡»è®¢é˜…æŒ‰é’®ï¼Œè·å–æ›´å¤šæ·±å…¥AIæ ¸å¿ƒçš„æ—…ç¨‹ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œå¼•å¯¼ä½ ç©¿è¶ŠæŠ€æœ¯ä¸–ç•Œçš„å¤æ‚æ€§ï¼

ğŸ‘‹ ä¸‹æ¬¡è§ï¼Œç»§ç»­æ¢ç´¢ï¼Œç»§ç»­åˆ›æ–°ï¼Œæ„¿ä½ çš„æ–‡æœ¬ç”Ÿæˆæ€»æ˜¯æ°åˆ°å¥½å¤„ï¼

---

[åŠ å…¥æˆ‘ä»¬ï¼Œç»§ç»­æ¢ç´¢AIç³»åˆ—çš„ä¸‹ä¸€é›†ï¼](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---

# ğŸ¬ Taking Control of LLMs: Crafting the Perfect Prompt for Next-Word Magic

Hey Tech Trailblazers! ğŸŒŸ Get ready to level up your AI game as we explore the art of guiding Large Language Models (LLMs) to generate text that's nothing short of extraordinary. Let's dive into the nitty-gritty of configuring these models to make them work their magic just the way you want!

## ğŸ”§ Mastering Model Configuration: The Keys to LLM Performance

Ever played around with LLMs on platforms like Hugging Face or AWS? You might've noticed those nifty controls that let you tweak the model's behavior. These aren't your regular training parameters; they're inference-time configurations that let you take the reins.

### ğŸš« Max New Tokens: The Token Generator's Cap
Think of **max new tokens** as a limit on the number of times the model can pick a word. It's like telling the model, "Hey, after this many words, it's time to wrap up!"

### ğŸŒ¡ï¸ Temperature: The Randomness Dial
Adjust the **temperature** to control randomness. A higher temp means more creative, but possibly wild, outputs. Lower it for more predictable text that sticks to what the model knows best.

## ğŸ° Next-Word Prediction: Beyond Greedy Decoding

By default, LLMs use **greedy decoding**, picking the word with the highest probability. But this can get repetitive. Time to mix things up!

### ğŸ° Random Sampling: Introducing Variability
With **random sampling**, the model picks words based on their probability scores, reducing repetition and adding a touch of unpredictability to the mix.

### ğŸ” Top K Sampling: The Cream of the Crop
Limit the model to the top **k** most probable words. It's like telling the model, "Only pick from the best of the best!"

### ğŸ¯ Top P Sampling: The Probability Pool
Or, use **top p** to consider only predictions that sum up to a certain probability. It's like saying, "Let's keep it sensible, shall we?"

## ğŸ› ï¸ Experimenting with Inference Parameters: The Lab of Text Generation

Playing with these parameters is like being a chef in a linguistic kitchenâ€”mixing and matching to create the perfect dish of text. But remember, the context window has limits, so make every word count!

## ğŸ“ˆ Scaling the Model's Understanding: Size Matters

The bigger the model, the better it gets at zero-shot inference, effortlessly handling tasks it wasn't explicitly trained for. Smaller models? They might need a bit more guidance.

## ğŸ”® Wrapping Up: Equipping Yourself with Configuration Superpowers

You've got the knowledge, now it's time to apply it. Whether you're fine-tuning or experimenting with prompts, you're on the path to mastering LLMs. In the next video, we'll take this a step further, exploring how to develop and launch an LLM-powered application.

Don't forget to hit that subscribe button for more journeys into the heart of AI. We're here to guide you through the complexities of the tech world!

ğŸ‘‹ Until next time, keep exploring, keep innovating, and may your text generations always be on point!

---

[Join us for the next episode in our AI exploration series!](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---

# ç§‘æ™®æŠ€æœ¯æ–‡ç« ï¼šå¦‚ä½•é€šè¿‡é…ç½®å‚æ•°ä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆ

## å¼•è¨€
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬çš„æ€§èƒ½å¯ä»¥é€šè¿‡ä¸€ç³»åˆ—é…ç½®å‚æ•°è¿›è¡Œå¾®è°ƒã€‚æœ¬æ–‡å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨è¿™äº›å‚æ•°æ¥å½±å“æ¨¡å‹ç”Ÿæˆä¸‹ä¸€ä¸ªè¯çš„å†³å®šã€‚

## é…ç½®å‚æ•°æ¦‚è§ˆ
åœ¨ä½¿ç”¨Hugging Faceç½‘ç«™æˆ–AWSç­‰å¹³å°æ—¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸€ç³»åˆ—æ§åˆ¶é€‰é¡¹æ¥è°ƒæ•´æ¨¡å‹çš„è¡Œä¸ºã€‚è¿™äº›å‚æ•°ä¸è®­ç»ƒæ—¶å­¦ä¹ çš„å‚æ•°ä¸åŒï¼Œå®ƒä»¬åœ¨æ¨ç†æ—¶è¢«è°ƒç”¨ï¼Œå…è®¸ç”¨æˆ·æ§åˆ¶è¾“å‡ºçš„é•¿åº¦å’Œåˆ›é€ æ€§ã€‚

## æœ€å¤§æ–°æ ‡è®°æ•°ï¼ˆMax New Tokensï¼‰
`max new tokens`å‚æ•°ç”¨äºé™åˆ¶æ¨¡å‹ç”Ÿæˆçš„æ ‡è®°æ•°é‡ã€‚ä¾‹å¦‚ï¼Œè®¾ç½®ä¸º100ã€150æˆ–200ï¼Œä½†å®é™…ç”Ÿæˆçš„é•¿åº¦å¯èƒ½å› æ¨¡å‹é¢„æµ‹åºåˆ—ç»“æŸæ ‡è®°è€Œæ›´çŸ­ã€‚

## è´ªå©ªè§£ç ä¸éšæœºé‡‡æ ·
é»˜è®¤æƒ…å†µä¸‹ï¼Œå¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹ä½¿ç”¨è´ªå©ªè§£ç ï¼ˆgreedy decodingï¼‰ï¼Œå³æ€»æ˜¯é€‰æ‹©æœ€é«˜æ¦‚ç‡çš„è¯ã€‚è¿™ç§æ–¹æ³•é€‚ç”¨äºçŸ­æ–‡æœ¬ç”Ÿæˆï¼Œä½†å¯èƒ½å¯¼è‡´é‡å¤çš„è¯æˆ–è¯åºåˆ—ã€‚éšæœºé‡‡æ ·ï¼ˆrandom samplingï¼‰é€šè¿‡æ¦‚ç‡åˆ†å¸ƒåŠ æƒéšæœºé€‰æ‹©è¾“å‡ºè¯ï¼Œå‡å°‘äº†é‡å¤çš„å¯èƒ½æ€§ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´æ–‡æœ¬åç¦»ä¸»é¢˜ã€‚

## Top Kå’ŒTop Pé‡‡æ ·æŠ€æœ¯
ä¸ºäº†åœ¨ä¿æŒéšæœºæ€§çš„åŒæ—¶æé«˜è¾“å‡ºçš„åˆç†æ€§ï¼Œå¯ä»¥ä½¿ç”¨Top Kå’ŒTop Pé‡‡æ ·æŠ€æœ¯ã€‚Top Kå‚æ•°é™åˆ¶æ¨¡å‹ä»…ä»æ¦‚ç‡æœ€é«˜çš„Kä¸ªæ ‡è®°ä¸­é€‰æ‹©ï¼Œè€ŒTop På‚æ•°é™åˆ¶æ¨¡å‹é€‰æ‹©çš„æ ‡è®°çš„ç´¯ç§¯æ¦‚ç‡ä¸è¶…è¿‡På€¼ã€‚

## æ¸©åº¦ï¼ˆTemperatureï¼‰å‚æ•°
æ¸©åº¦å‚æ•°å½±å“æ¨¡å‹è®¡ç®—ä¸‹ä¸€ä¸ªæ ‡è®°çš„æ¦‚ç‡åˆ†å¸ƒçš„å½¢çŠ¶ã€‚è¾ƒé«˜çš„æ¸©åº¦å€¼å¢åŠ éšæœºæ€§ï¼Œè€Œè¾ƒä½çš„æ¸©åº¦å€¼å‡å°‘éšæœºæ€§ã€‚é€šè¿‡è°ƒæ•´æ¸©åº¦ï¼Œå¯ä»¥æ§åˆ¶æ–‡æœ¬ç”Ÿæˆçš„åˆ›é€ æ€§å’Œå¯é¢„æµ‹æ€§ã€‚

## ç»“è¯­
é€šè¿‡æœ¬æ–‡ï¼Œæˆ‘ä»¬äº†è§£äº†å¦‚ä½•ä½¿ç”¨ä¸åŒçš„é…ç½®å‚æ•°æ¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆæ€§èƒ½ã€‚ä»è´ªå©ªè§£ç åˆ°éšæœºé‡‡æ ·ï¼Œå†åˆ°Top Kã€Top På’Œæ¸©åº¦å‚æ•°çš„è°ƒæ•´ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®éœ€è¦ç”Ÿæˆæ›´å‡†ç¡®ã€æ›´æœ‰åˆ›é€ æ€§æˆ–æ›´åˆç†çš„æ–‡æœ¬ã€‚

---

æœ¬æ–‡ä¸ºè¯»è€…æä¾›äº†æ·±å…¥ç†è§£è¯­è¨€æ¨¡å‹é…ç½®å‚æ•°çš„æœºä¼šï¼Œå¸®åŠ©ä»–ä»¬æ›´æœ‰æ•ˆåœ°åˆ©ç”¨è¿™äº›å¼ºå¤§çš„å·¥å…·æ¥ç”ŸæˆæœŸæœ›çš„æ–‡æœ¬è¾“å‡ºã€‚

---

[åŠ å…¥æˆ‘ä»¬ï¼Œç»§ç»­æ¢ç´¢AIç³»åˆ—çš„ä¸‹ä¸€é›†ï¼](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---
