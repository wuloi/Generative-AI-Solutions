# ğŸ§  ç²¾é€šLLMsçš„æç¤ºå·¥ç¨‹è‰ºæœ¯ï¼šæ·±åº¦è§£æ

å˜¿ï¼ŒæŠ€æœ¯å°é˜Ÿï¼ğŸ‘‹ è®©æˆ‘ä»¬ä¸€èµ·æ·±å…¥æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥åŠæç¤ºçš„å¼ºå¤§åŠ›é‡ã€‚æ— è®ºä½ æ˜¯èµ„æ·±çš„ç¼–ç è€…è¿˜æ˜¯å¥½å¥‡çš„çµé­‚ï¼Œè¿™éƒ½æ˜¯ä¸€æ®µä½ ä¸æƒ³é”™è¿‡çš„æŠ€æœ¯ä¹‹æ—…ï¼

## ğŸ“ LLMsçš„è¯­è¨€ï¼šæç¤ºã€æ¨ç†å’Œè¡¥å…¨
æƒ³çŸ¥é“å¦‚ä½•ä¸LLMå¯¹è¯å—ï¼Ÿä¸€åˆ‡ä»**æç¤º**å¼€å§‹ã€‚å½“æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œè¿™è¢«ç§°ä¸º**æ¨ç†**ï¼Œä½ å¾—åˆ°çš„è¾“å‡ºç§°ä¸º**è¡¥å…¨**ã€‚ä½ çš„æç¤ºçš„æ¸¸ä¹åœºæ˜¯**ä¸Šä¸‹æ–‡çª—å£**ï¼Œé‚£é‡Œå°±æ˜¯é­”æ³•å‘ç”Ÿçš„åœ°æ–¹ã€‚

## ğŸ”§ æç¤ºå·¥ç¨‹ï¼šä¸AIæ²Ÿé€šçš„ç²¾ç»†è‰ºæœ¯
æ²¡å¾—åˆ°ä½ æƒ³è¦çš„è¾“å‡ºï¼Ÿåˆ«æ‹…å¿ƒï¼æ˜¯æ—¶å€™è¿›è¡Œä¸€äº›**æç¤ºå·¥ç¨‹**äº†ã€‚ä½ å¯èƒ½éœ€è¦è°ƒæ•´ä½ çš„è¯­è¨€æˆ–ç»“æ„ï¼Œç›´åˆ°æ¨¡å‹è¡¨ç°æ°åˆ°å¥½å¤„ã€‚

### ğŸ“š ä¸Šä¸‹æ–‡å­¦ä¹ ï¼šä»¥ä¾‹æ•™å­¦
ä¸€ä¸ªå¼ºå¤§çš„ç­–ç•¥æ˜¯**ä¸Šä¸‹æ–‡å­¦ä¹ **ã€‚å¯ä»¥æƒ³è±¡ä¸€ä¸‹ï¼Œé€šè¿‡åœ¨æç¤ºæœ¬èº«ä¸­åŒ…å«ä»»åŠ¡çš„ä¾‹å­ï¼Œå°±åƒæ˜¯ç»™æ¨¡å‹ä¸€å¼ ä½œå¼Šçº¸ã€‚

## ğŸ¯ é›¶æ ·æœ¬æ¨ç†ï¼šLLMçš„æ°´æ™¶çƒ
é›¶æ ·æœ¬æ¨ç†ï¼Œä½ åŸºæœ¬ä¸Šæ˜¯åœ¨è¯´ï¼Œâ€œå˜¿AIï¼Œä¸ç”¨ä»»ä½•ä¾‹å­å°±è§£å†³è¿™ä¸ªé—®é¢˜ï¼â€ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæœ€å¤§çš„LLMåœ¨è¿™æ–¹é¢ç›¸å½“æ“…é•¿ï¼Œèƒ½å¤Ÿç†è§£ä»»åŠ¡å¹¶æä¾›å‡†ç¡®çš„å“åº”ã€‚

## ğŸŒ° å•æ ·æœ¬å’Œå°‘æ ·æœ¬æ¨ç†ï¼šé€šè¿‡ä¾‹å­å­¦ä¹ 
ä½†å¦‚æœä½ è¦å¤„ç†çš„æ˜¯ä¸€ä¸ªè¡¨ç°ä¸ä½³çš„è¾ƒå°LLMå‘¢ï¼Ÿè¿™å°±æ˜¯**å•æ ·æœ¬**æˆ–**å°‘æ ·æœ¬æ¨ç†**çš„ç”¨æ­¦ä¹‹åœ°ã€‚é€šè¿‡æä¾›å•ä¸€ä¾‹å­æˆ–å¤šä¸ªä¾‹å­ï¼Œä½ å¼•å¯¼æ¨¡å‹æ›´å¥½åœ°ç†è§£ä»»åŠ¡ã€‚

### ğŸ”„ å¾®è°ƒï¼šå½“ä¾‹å­ä¸å¤Ÿç”¨æ—¶
å¦‚æœç”¨ä¾‹å­è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ è¿˜ä¸å¤Ÿï¼Œå¯èƒ½å°±éœ€è¦å¯¹ä½ çš„æ¨¡å‹è¿›è¡Œ**å¾®è°ƒ**äº†ã€‚è¿™æ¶‰åŠåˆ°ä½¿ç”¨æ–°æ•°æ®è¿›è¡Œé¢å¤–è®­ç»ƒï¼Œè®©ä½ çš„æ¨¡å‹æˆä¸ºä»»åŠ¡å¤§å¸ˆã€‚

## ğŸ” æ¨¡å‹çš„è§„æ¨¡ï¼šå¤§å°å¾ˆé‡è¦
ä½ çš„æ¨¡å‹å¤§å°å¯ä»¥å†³å®šå®ƒåœ¨å¤šä¸ªä»»åŠ¡ä¸­çš„è¡¨ç°å¦‚ä½•ã€‚æ‹¥æœ‰æ›´å¤šå‚æ•°çš„è¾ƒå¤§æ¨¡å‹å°±åƒæ˜¯è¯­è¨€å¤§å¸ˆï¼Œæ“…é•¿é›¶æ ·æœ¬æ¨ç†ã€‚

## ğŸ›ï¸ é…ç½®è®¾ç½®ï¼šå¼•å¯¼æ¨¡å‹çš„åˆ›é€ åŠ›
æ‰¾åˆ°äº†ä½ çš„æ¨¡å‹ï¼Œä½†å®ƒè¿˜æ²¡æœ‰å®Œå…¨è¾¾åˆ°ç›®æ ‡ï¼Ÿå°è¯•è°ƒæ•´é…ç½®è®¾ç½®ï¼Œä»¥å½±å“æ¨¡å‹è¡¥å…¨çš„ç»“æ„å’Œé£æ ¼ã€‚

## ğŸ”® æ€»ç»“ï¼šLLMsä½œä¸ºå¤šåŠŸèƒ½å·¥å…·
è®°ä½ï¼Œä¸LLMsåˆä½œçš„å…³é”®æ˜¯äº†è§£å¦‚ä½•åˆ¶ä½œå¾—åˆ°ä½ æƒ³è¦çš„ç»“æœçš„æç¤ºã€‚æ— è®ºæ˜¯é›¶æ ·æœ¬ã€å•æ ·æœ¬ã€å°‘æ ·æœ¬è¿˜æ˜¯å¾®è°ƒï¼Œä½ éƒ½æ‹¥æœ‰å¼•å¯¼AIè¾¾åˆ°ä½ æœŸæœ›ç»“æœçš„åŠ›é‡ã€‚

åˆ«å¿˜äº†ç‚¹å‡»è®¢é˜…æŒ‰é’®ï¼Œè·å–æ›´å¤šæ·±å…¥æŠ€æœ¯é¢†åŸŸçš„æ´å¯Ÿã€‚æˆ‘ä»¬åœ¨è¿™é‡Œç…§äº®ä½ é€šå¾€AIæŒæ¡ä¹‹è·¯ï¼

ğŸ‘‹ ä¸‹æ¬¡è§ï¼Œç»§ç»­å®éªŒï¼Œç»§ç»­åˆ›æ–°ï¼Œæ„¿ä½ çš„æç¤ºæ€»æ˜¯æ°åˆ°å¥½å¤„ï¼

---

[åŠ å…¥æˆ‘ä»¬ï¼Œæ·±å…¥äº†è§£AIä¸–ç•Œï¼](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---

# ğŸ§  Mastering the Art of Prompt Engineering with LLMs: A Deep Dive

Hey Tech Squad! ğŸ‘‹ Let's dive into the fascinating world of Large Language Models (LLMs) and the power of prompts. Whether you're a seasoned coder or a curious mind, this is one tech journey you won't want to miss!

## ğŸ“ The Language of LLMs: Prompts, Inference, and Completions
Ever wondered how you talk to an LLM? It all starts with a **prompt**. When the model generates text, it's called **inference**, and the output you get is known as the **completion**. The playground for your prompt is the **context window**, and it's where the magic happens.

## ğŸ”§ Prompt Engineering: The Fine Art of Communicating with AI
Not getting the output you want? No worries! It's time for some **prompt engineering**. You might need to tweak your language or structure until you get the model to perform just right.

### ğŸ“š In-Context Learning: Teaching by Example
One powerful strategy is **in-context learning**. Think of it as giving the model a cheat sheet by including examples of the task within the prompt itself.

## ğŸ¯ Zero-Shot Inference: The LLM's Crystal Ball
With zero-shot inference, you're basically saying, "Hey AI, figure this out without any examples!" And surprisingly, the largest LLMs are pretty good at it, grasping the task and delivering accurate responses.

## ğŸŒ° One-Shot and Few-Shot Inference: Learning by Examples
But what if you're working with a smaller LLM that's struggling? That's where **one-shot** or **few-shot inference** comes in. By providing a single example or multiple examples, you guide the model to understand the task better.

### ğŸ”„ Fine-Tuning: When Examples Aren't Enough
If in-context learning with examples isn't cutting it, it might be time to **fine-tune** your model. This involves additional training with new data to make your model a task master.

## ğŸ” The Scale of Models: Size Matters
The size of your model can determine how well it performs across multiple tasks. Larger models with more parameters are like language virtuosos, excelling at zero-shot inference.

## ğŸ›ï¸ Configuration Settings: Steering the Model's Creativity
Found your model, but it's not quite hitting the mark? Play around with configuration settings to influence the structure and style of the model's completions.

## ğŸ”® Wrapping Up: LLMs as Versatile Tools
Remember, the key to working with LLMs is understanding how to craft prompts that get the results you want. Whether it's zero-shot, one-shot, few-shot, or fine-tuning, you hold the power to guide the AI to your desired outcome.

Don't forget to hit that subscribe button for more deep dives into the tech realm. We're here to illuminate your path to AI mastery!

ğŸ‘‹ Until next time, keep experimenting, keep innovating, and may your prompts always be on point!

---

[Join us for more insights into the world of AI!](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---

# ç§‘æ™®æŠ€æœ¯æ–‡ç« ï¼šæå‡è¯­è¨€æ¨¡å‹æ€§èƒ½çš„ç­–ç•¥â€”â€”æç¤ºå·¥ç¨‹ä¸ä¸Šä¸‹æ–‡å­¦ä¹ 

## å¼•è¨€
åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸï¼Œè¯­è¨€æ¨¡å‹çš„ç²¾å‡†é¢„æµ‹ä¾èµ–äºæœ‰æ•ˆçš„æç¤ºï¼ˆpromptï¼‰è®¾è®¡ã€‚æœ¬æ–‡å°†æ¢è®¨å¦‚ä½•é€šè¿‡æç¤ºå·¥ç¨‹å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ¥ä¼˜åŒ–æ¨¡å‹çš„è¾“å‡ºç»“æœã€‚

## æç¤ºã€æ¨ç†å’Œå®Œæˆ
åœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œè¾“å…¥æ¨¡å‹çš„æ–‡æœ¬ç§°ä¸ºæç¤ºï¼ˆpromptï¼‰ï¼Œç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹ç§°ä¸ºæ¨ç†ï¼ˆinferenceï¼‰ï¼Œè€Œè¾“å‡ºçš„æ–‡æœ¬ç§°ä¸ºå®Œæˆï¼ˆcompletionï¼‰ã€‚

## ä¸Šä¸‹æ–‡çª—å£ä¸æç¤ºå·¥ç¨‹
ä¸Šä¸‹æ–‡çª—å£ï¼ˆcontext windowï¼‰æ˜¯æ¨¡å‹ç”¨æ¥ç†è§£æç¤ºçš„æ–‡æœ¬æˆ–è®°å¿†å®¹é‡ã€‚æç¤ºå·¥ç¨‹ï¼ˆprompt engineeringï¼‰æ˜¯å¼€å‘å’Œæ”¹è¿›æç¤ºçš„è¿‡ç¨‹ï¼Œä»¥ä½¿æ¨¡å‹æŒ‰ç…§é¢„æœŸè¡Œä¸ºã€‚

## é›¶æ ·æœ¬æ¨ç†ä¸ä¸Šä¸‹æ–‡å­¦ä¹ 
é›¶æ ·æœ¬æ¨ç†ï¼ˆzero-shot inferenceï¼‰æ˜¯æ¨¡å‹åœ¨æ²¡æœ‰ç¤ºä¾‹çš„æƒ…å†µä¸‹ç†è§£ä»»åŠ¡å¹¶è¿”å›ç­”æ¡ˆçš„èƒ½åŠ›ã€‚è€Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆin-context learningï¼‰æ˜¯é€šè¿‡åœ¨æç¤ºä¸­åŒ…å«ä»»åŠ¡ç¤ºä¾‹æ¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£ä»»åŠ¡ã€‚

## å•æ ·æœ¬ä¸å°‘æ ·æœ¬æ¨ç†
å•æ ·æœ¬æ¨ç†ï¼ˆone-shot inferenceï¼‰å’Œå°‘æ ·æœ¬æ¨ç†ï¼ˆfew-shot inferenceï¼‰æ˜¯é€šè¿‡åœ¨æç¤ºä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªç¤ºä¾‹æ¥æŒ‡å¯¼æ¨¡å‹ç†è§£æ‰€éœ€æ‰§è¡Œçš„ä»»åŠ¡ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¸®åŠ©å°å‹æ¨¡å‹åœ¨ç†è§£ä»»åŠ¡å’Œæ ¼å¼ä¸Šè¡¨ç°å¾—æ›´å¥½ã€‚

## æ¨¡å‹è§„æ¨¡ä¸å¤šä»»åŠ¡æ€§èƒ½
æ¨¡å‹çš„è§„æ¨¡å¯¹å¤šä»»åŠ¡æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ã€‚å¤§å‹æ¨¡å‹é€šå¸¸åœ¨é›¶æ ·æœ¬æ¨ç†ä¸­è¡¨ç°è‰¯å¥½ï¼Œè€Œå°å‹æ¨¡å‹å¯èƒ½ä»…åœ¨å®ƒä»¬è®­ç»ƒè¿‡çš„ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚

## å¾®è°ƒï¼šæå‡æ¨¡å‹æ€§èƒ½
å½“æ¨¡å‹åœ¨åŒ…å«å¤šä¸ªç¤ºä¾‹çš„æƒ…å†µä¸‹è¡¨ç°ä¸ä½³æ—¶ï¼Œå¾®è°ƒï¼ˆfine-tuningï¼‰å¯ä»¥ä½œä¸ºä¸€ç§æå‡æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ã€‚é€šè¿‡ä½¿ç”¨æ–°æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œé¢å¤–è®­ç»ƒï¼Œä½¿å…¶æ›´æ“…é•¿æ‰§è¡Œç‰¹å®šä»»åŠ¡ã€‚

## ç»“è¯­
é€šè¿‡æœ¬æ–‡ï¼Œæˆ‘ä»¬äº†è§£åˆ°äº†å¦‚ä½•é€šè¿‡æç¤ºå·¥ç¨‹ã€ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå¾®è°ƒæ¥æå‡è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è™½ç„¶å¤§å‹æ¨¡å‹åœ¨å¤„ç†å¤šç§ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†å°å‹æ¨¡å‹ä¹Ÿå¯ä»¥é€šè¿‡é€‚å½“çš„ç¤ºä¾‹å’Œå¾®è°ƒæ¥æé«˜å…¶æ€§èƒ½ã€‚

---

æœ¬æ–‡ä»¥é€šä¿—æ˜“æ‡‚çš„æ–¹å¼ä»‹ç»äº†å¦‚ä½•é€šè¿‡ä¸åŒçš„ç­–ç•¥æ¥æå‡è¯­è¨€æ¨¡å‹çš„è¾“å‡ºè´¨é‡ï¼Œæ—¨åœ¨å¸®åŠ©è¯»è€…ç†è§£å¹¶åº”ç”¨è¿™äº›ç­–ç•¥æ¥ä¼˜åŒ–æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚

---
