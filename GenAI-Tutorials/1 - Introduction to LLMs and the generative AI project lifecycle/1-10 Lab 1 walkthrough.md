# ğŸ§ª å®éªŒ1å¯åŠ¨ï¼šæ‰‹æŠŠæ‰‹ä½¿ç”¨LLMè¿›è¡Œå¯¹è¯æ‘˜è¦

å˜¿ï¼ŒæŠ€æœ¯æ¢é™©å®¶ä»¬ï¼ğŸŒŸ ç©¿ä¸Šå®éªŒæœï¼Œæˆ‘ä»¬å°†æ·±å…¥å®éªŒ1ï¼Œæˆ‘ä»¬å°†æŠŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çŸ¥è¯†åº”ç”¨åˆ°å®é™…ä¸­ï¼šæ€»ç»“å®¢æˆ·æ”¯æŒå¯¹è¯ã€‚æ˜¯æ—¶å€™å°†åŸå§‹å¯¹è¯è½¬æ¢ä¸ºç®€æ´ã€å¯Œæœ‰æ´å¯ŸåŠ›çš„æ‘˜è¦äº†ï¼

## ğŸš€ æ­å»ºèˆå°ï¼šä½ çš„LLMå®éªŒå®¤ç¯å¢ƒ
æˆ‘ä»¬çš„å®éªŒå®¤å¤§å¸ˆChris Fragleyå·²åœ¨Vocareumä¸ºæˆ‘ä»¬æ­å»ºäº†èˆå°ï¼Œè¿™æ˜¯ä¸€ä¸ªè®©ä½ å…è´¹äº²èº«ä½“éªŒAWSå’ŒAmazon SageMakerçš„å¹³å°ï¼

### ğŸ› ï¸ ç³»ç»Ÿæ£€æŸ¥ï¼šPythonã€PyTorchç­‰ç­‰
- **è§„æ ¼æ£€æŸ¥**ï¼šå…«æ ¸CPUã€32GB RAMã€Python 3 â€” æˆ‘ä»¬å‡†å¤‡å……åˆ†ï¼
- **åŠ è½½åº“**ï¼šPyTorchã€Torchæ•°æ®ï¼Œä»¥åŠæ¥è‡ªHugging Faceçš„Transformersåº“ã€‚è¿™äº›æ˜¯ä½ çš„å·¥å…·ã€‚

### ğŸ“š è®¤è¯†æ•°æ®é›†ï¼šå¯¹è¯æ‘˜è¦
æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ä¸€ä¸ªä¸ºå¯¹è¯æ‘˜è¦è®¾è®¡çš„å…¬å…±æ•°æ®é›†ã€‚è¿™æ˜¯ä½ ç”¨LLMsè¿›è¡Œå®éªŒçš„æ¸¸ä¹åœºã€‚

### ğŸ”§ å®éªŒè®¾ç½®ï¼šä»å®‰è£…åˆ°å¯¼å…¥
- **å®‰è£…å¯åŠ¨**ï¼šè§‚å¯Ÿpipå®‰è£…å¿…è¦çš„åº“ã€‚
- **å¯¼å…¥101**ï¼šåŠ è½½å®éªŒå®¤æ‰€éœ€çš„å‡½æ•°ã€æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚

## ğŸ¤– æ¨¡å‹è§é¢ä¼šï¼šFLAN-T5ç™»å°
å¤šåŠŸèƒ½çš„LLM FLAN-T5ï¼Œå³å°†æ‰¿æ‹…æ€»ç»“å¯¹è¯çš„ä»»åŠ¡ã€‚ä½†é¦–å…ˆï¼Œå®ƒéœ€è¦è¢«åŠ è½½å’Œåˆ†è¯ã€‚

### ğŸ“ˆ åˆ†è¯å™¨çš„è§’è‰²ï¼šä»æ–‡æœ¬åˆ°å‘é‡
åˆ†è¯å™¨çš„å·¥ä½œï¼Ÿå°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°å­—æ ¼å¼ã€‚

## ğŸ”­ æ¢ç´¢æ•°æ®ï¼šå¯¹è¯ä¸€ç¥
çœ‹çœ‹å¯¹è¯æ ·æœ¬åŠå…¶äººå·¥ç”Ÿæˆçš„æ‘˜è¦ã€‚è¿™æ˜¯ä½ çš„åŸºçº¿ â€” ä½ çš„æ¨¡å‹å°†åŠªåŠ›è¾¾åˆ°æˆ–è¶…è¶Šçš„æ ‡å‡†ã€‚

## ğŸ“ æ‘˜è¦å¯¹å†³ï¼šæ¨¡å‹ä¸äººç±»
è®©æˆ‘ä»¬çœ‹çœ‹æ¨¡å‹ä¸äººç±»æ‘˜è¦çš„è¡¨ç°å¦‚ä½•ã€‚å®ƒä¼šåŒ¹é…ã€è¶…è¶Šè¿˜æ˜¯ä¸è¶³ï¼Ÿ

### ğŸŒ¡ï¸ ç©è½¬æç¤ºï¼šä¸Šä¸‹æ–‡å­¦ä¹ 
- **é›¶æ ·æœ¬æ¨ç†**ï¼šç»™å‡ºä¸€ä¸ªæ²¡æœ‰ä¾‹å­çš„æç¤ºï¼Œçœ‹çœ‹å®ƒç”Ÿæˆäº†ä»€ä¹ˆã€‚
- **å•æ ·æœ¬å­¦ä¹ **ï¼šå±•ç¤ºä¸€ä¸ªæ­£ç¡®çš„ç¤ºä¾‹ï¼Œç„¶åè®©å®ƒæ‰§è¡Œã€‚
- **å°‘æ ·æœ¬æ¨ç†**ï¼šç»™å‡ºå¤šä¸ªæ­£ç¡®çš„ç¤ºä¾‹ä¾›å…¶å­¦ä¹ ã€‚

## ğŸ›ï¸ åå¤è°ƒæ•´ï¼šé…ç½®å‚æ•°
ç°åœ¨æ˜¯æœ€æœ‰è¶£çš„éƒ¨åˆ† â€” å°è¯•æ¨¡å‹çš„é…ç½®å‚æ•°ï¼Œå¦‚é‡‡æ ·å’Œæ¸©åº¦ï¼Œçœ‹çœ‹å®ƒä»¬å¦‚ä½•å½±å“æ¨¡å‹çš„è¾“å‡ºã€‚

### ğŸ§© å®éªŒç«™ï¼šæ‰¾åˆ°æ­£ç¡®çš„æç¤º
è·å¾—å‡ºè‰²LLMæ€§èƒ½çš„å…³é”®åœ¨äºæç¤ºã€‚æµ‹è¯•ä¸åŒçš„æç¤ºï¼Œçœ‹çœ‹å“ªä¸ªèƒ½äº§ç”Ÿæœ€ä½³ç»“æœã€‚

## ğŸ”® æ€»ç»“ï¼šLLMå®éªŒå®¤æ¢ç´¢
å®éªŒ1æ˜¯ä½ æŒæ¡LLMå®é™…åº”ç”¨çš„ç¬¬ä¸€æ­¥ã€‚è®°ä½ï¼Œå®éªŒæ˜¯æˆåŠŸçš„å…³é”®ã€‚ä¸æ–­å°è¯•ä¸åŒçš„æ–¹æ³•ï¼Œç›´åˆ°æ‰¾åˆ°æœ€ä½³ç‚¹ã€‚

åˆ«å¿˜äº†ç‚¹å‡»è®¢é˜…æŒ‰é’®ï¼Œè·å–æ›´å¤šæŠ€æœ¯ä¸–ç•Œçš„åŠ¨æ‰‹å†’é™©ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œå¼•å¯¼ä½ ç©¿è¶Šç¼–ç åŠå…¶ä¹‹å¤–çš„å¤æ‚æ€§ï¼

ğŸ‘‹ ä¸‹æ¬¡è§ï¼Œç»§ç»­å®éªŒï¼Œç»§ç»­å­¦ä¹ ï¼Œæ„¿ä½ çš„æ‘˜è¦æ€»æ˜¯æ°åˆ°å¥½å¤„ï¼

---

[åŠ å…¥æˆ‘ä»¬ï¼Œæ¢ç´¢æ›´å¤šç¼–ç å’ŒæŠ€æœ¯ï¼](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---

# ğŸ§ª Lab 1 Launch: Hands-on with LLMs for Dialogue Summarization

Hey Tech Explorers! ğŸŒŸ Get your lab coats on as we dive into Lab 1, where we'll put our Large Language Model (LLM) knowledge into action with a real-world application: summarizing customer support dialogues. It's time to transform raw conversations into concise, insightful summaries!

## ğŸš€ Setting the Stage: Your LLM Lab Environment
Chris Fragley, our lab maestro, has set the stage for us in Vocareum, a platform that gives you a hands-on AWS experience with Amazon SageMaker â€” all for free!

### ğŸ› ï¸ System Check: Python, PyTorch, and More
- **Specs Check**: Eight CPUs, 32GB RAM, Python 3 â€” we're cooking with gas!
- **Libraries Loading**: PyTorch, Torch data, and the Transformers library from Hugging Face. These are your tools for the job.

### ğŸ“š Meet the Dataset: Dialogue Sum
We're working with a public dataset designed for dialogue summarization. It's your playground for experimenting with LLMs.

### ğŸ”§ Lab Setup: From Installs to Imports
- **Installation Inception**: Watch as pip installs the necessary libraries.
- **Imports 101**: Load up the functions, models, and tokenizers needed for the lab.

## ğŸ¤– Model Meetup: FLAN-T5 Takes the Stage
FLAN-T5, the versatile LLM, is up for the task of summarizing conversations. But first, it needs to be loaded and tokenized.

### ğŸ“ˆ Tokenizer's Role: From Text to Vectors
The tokenizer's job? Convert raw text into a numerical format that the model can munch on.

## ğŸ”­ Exploring the Data: A Glimpse into Dialogues
Take a peek at the dialogue samples and their human-generated summaries. This is your baseline â€” the standard your model will strive to meet or beat.

## ğŸ“ Summarization Showdown: Model vs. Human
Let's see how the model performs against the human summaries. Will it match, exceed, or fall short?

### ğŸŒ¡ï¸ Playing with Prompts: In-Context Learning
- **Zero-Shot Inference**: Give it a prompt with no examples, see what it generates.
- **One-Shot Learning**: Show it one correct example, then ask it to perform.
- **Few-Shot Inference**: Give it multiple correct examples to learn from.

## ğŸ›ï¸ Tweak and Tweak Again: Configuration Parameters
Now's the fun part â€” play with the model's configuration parameters like sampling and temperature to see how they affect the model's output.

### ğŸ§© Experimentation Station: Finding the Right Prompt
The key to great LLM performance is often in the prompt. Test different prompts to see which yields the best results.

## ğŸ”® Wrapping Up: LLM Lab Exploration
Lab 1 is your first step in mastering LLMs for practical applications. Remember, experimentation is the key to success. Keep trying different approaches until you hit the sweet spot.

Don't forget to hit that subscribe button for more hands-on adventures in the world of tech. We're here to guide you through the complexities of coding and beyond!

ğŸ‘‹ Until next time, keep experimenting, keep learning, and may your summaries always be on point!

---

[Join us for more coding and tech explorations!](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---

# ç§‘æ™®æŠ€æœ¯æ–‡ç« ï¼šæ¢ç´¢å¯¹è¯æ‘˜è¦ç”Ÿæˆçš„å®éªŒå®¤å®è·µ

## å¼•è¨€
åœ¨æœ¬å®éªŒå®¤ç»ƒä¹ ä¸­ï¼Œä½ å°†é€šè¿‡å®é™…æ“ä½œä»£ç æ¥å·©å›ºä½ å¯¹ç”Ÿæˆå¼AIçš„ç†è§£ã€‚æˆ‘ä»¬å°†ä¸“æ³¨äºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥æ‘˜è¦å¯¹è¯æ•°æ®é›†ï¼Œä¾‹å¦‚å®¢æˆ·æ”¯æŒå¯¹è¯ã€‚

## å®éªŒå®¤ç¯å¢ƒè®¾ç½®
é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Vocareumå®éªŒå®¤ç¯å¢ƒï¼Œå®ƒæä¾›äº†ä¸€ä¸ªAWSè´¦æˆ·ï¼Œè®©ä½ èƒ½å¤Ÿå…è´¹ä½¿ç”¨Amazon SageMakeræ¥è¿è¡ŒJupyterç¬”è®°æœ¬ã€‚

## ç¯å¢ƒä¸ä¾èµ–é¡¹
åœ¨å®éªŒå®¤ä¸­ï¼Œä½ å°†æ‹¥æœ‰8ä¸ªCPUæ ¸å¿ƒå’Œ32GBçš„RAMï¼Œè¿è¡ŒPython 3ã€‚æˆ‘ä»¬å°†å®‰è£…PyTorchåŠå…¶ç›¸å…³åº“ï¼ŒåŒ…æ‹¬Torchæ•°æ®å’ŒHugging Faceçš„Transformersåº“ï¼Œè¿™äº›åº“æä¾›äº†å¤§é‡å¼€æºå·¥å…·ç”¨äºå¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹ã€‚

## æ•°æ®é›†ä¸æ¨¡å‹
æˆ‘ä»¬å°†ä½¿ç”¨åä¸ºâ€œDialogue sumâ€çš„å…¬å…±æ•°æ®é›†ï¼Œå®ƒé€šè¿‡Transformersåº“ä¸­çš„data-setså·¥å…·æä¾›ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…å«äº†å¯¹è¯å’Œäººç±»ç”Ÿæˆçš„æ‘˜è¦ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™äº›æ•°æ®æ¥è®­ç»ƒå’Œæµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹ã€‚

## æ¨¡å‹ä¸åˆ†è¯å™¨
æˆ‘ä»¬å°†ä½¿ç”¨FLAN-T5æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¨¡å‹ï¼Œèƒ½å¤Ÿæ‰§è¡Œå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¯¹è¯æ‘˜è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†åŠ è½½åˆ†è¯å™¨ï¼Œå®ƒè´Ÿè´£å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°å€¼å‘é‡ã€‚

## åˆæ­¥å°è¯•ä¸æ¨¡å‹ç”Ÿæˆæ‘˜è¦
åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨åï¼Œæˆ‘ä»¬å°†å°è¯•ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå¯¹è¯çš„æ‘˜è¦ã€‚åˆæ­¥å°è¯•å¯èƒ½ä¸ä¼šå¾ˆå®Œç¾ï¼Œä½†è¿™æ˜¯å­¦ä¹ è¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚

## æå‡æ¨¡å‹æ€§èƒ½
ä¸ºäº†æå‡æ¨¡å‹æ€§èƒ½ï¼Œæˆ‘ä»¬å°†æ¢ç´¢ä¸åŒçš„æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ï¼ˆzero-shotï¼‰ã€å•æ ·æœ¬ï¼ˆone-shotï¼‰å’Œå°‘æ ·æœ¬ï¼ˆfew-shotï¼‰æ¨ç†ã€‚è¿™äº›æŠ€æœ¯é€šè¿‡æä¾›ä¸åŒçš„æŒ‡ä»¤æˆ–ç¤ºä¾‹æ¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£ä»»åŠ¡ã€‚

## é…ç½®å‚æ•°è°ƒæ•´
æœ€åï¼Œä½ å°†æœ‰æœºä¼šè°ƒæ•´æ¨¡å‹ç”Ÿæˆçš„é…ç½®å‚æ•°ï¼Œå¦‚é‡‡æ ·å’Œæ¸©åº¦ï¼Œæ¥è§‚å¯Ÿè¿™äº›å‚æ•°å¦‚ä½•å½±å“æ¨¡å‹è¾“å‡ºçš„åˆ›é€ æ€§å’Œä¿å®ˆæ€§ã€‚

## ç»“è¯­
é€šè¿‡æœ¬æ–‡ï¼Œä½ å·²ç»äº†è§£äº†å¦‚ä½•åœ¨å®éªŒå®¤ç¯å¢ƒä¸­è®¾ç½®å’Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆå¯¹è¯æ‘˜è¦ã€‚é€šè¿‡å®è·µï¼Œä½ å°†åŠ æ·±å¯¹æ¨¡å‹ã€åˆ†è¯å™¨ã€æç¤ºå·¥ç¨‹å’Œé…ç½®å‚æ•°è°ƒæ•´çš„ç†è§£ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åº”ç”¨è¿™äº›æŠ€æœ¯ã€‚

---

æœ¬æ–‡ä¸ºè¯»è€…æä¾›äº†ä¸€ä¸ªå®è·µæŒ‡å—ï¼Œå¸®åŠ©ä»–ä»¬åœ¨ç”Ÿæˆå¼AIé¢†åŸŸä¸­é€šè¿‡å®éªŒå®¤ç»ƒä¹ æ¥æé«˜æŠ€èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è¯æ‘˜è¦ç”Ÿæˆçš„ä»»åŠ¡ä¸­ã€‚

---

[åŠ å…¥æˆ‘ä»¬ï¼Œæ¢ç´¢æ›´å¤šç¼–ç å’ŒæŠ€æœ¯ï¼](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---
