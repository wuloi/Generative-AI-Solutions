# ğŸŒŸ å¤§å‹æ¨¡å‹çš„å¹³è¡¡æœ¯ï¼šå¯»æ‰¾è®¡ç®—æœ€ä¼˜çš„LLMs

å˜¿ï¼ŒæŠ€æœ¯ä¸“å®¶ï¼ğŸ§™â€â™€ï¸ åŠ å…¥æˆ‘ä»¬ï¼Œä¸€èµ·æ¢ç´¢è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è®¡ç®—é­…åŠ›ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†æ­å¼€æ¨¡å‹å¤§å°ã€æ•°æ®é›†å¹¿åº¦å’Œç¥ç§˜è®¡ç®—é¢„ç®—çš„ç§˜å¯†ã€‚å‡†å¤‡å¥½åˆ©ç”¨æœ€ä¼˜é¢„è®­ç»ƒçš„åŠ›é‡å§ï¼

## ğŸ“Š è§„æ¨¡çš„åŠ›é‡ï¼šæ•°æ®é›†å’Œå‚æ•°
åœ¨è¿½æ±‚é¢„è®­ç»ƒè‡³é«˜æ— ä¸Šçš„è·¯ä¸Šï¼Œä½ æ‹¥æœ‰ä¸¤ä¸ªå¼ºå¤§çš„æ­¦å™¨ï¼šä½ çš„æ•°æ®é›†å¤§å°å’Œæ¨¡å‹ä¸­çš„å‚æ•°æ•°é‡ã€‚ç†è®ºä¸Šï¼Œå¤šå¤šç›Šå–„ï¼Œä½†è¦å°å¿ƒè®¡ç®—é¢„ç®—çš„å·¨é¾™ï¼

### ğŸ§­ èˆªè¡Œåœ¨è®¡ç®—é¢„ç®—çš„æµ·æ´‹
æ¯ç§’åƒä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®—æ˜¯ä½ çš„å…­åˆ†ä»ªï¼Œè§„åˆ’ç€ä½ çš„GPUèˆ°é˜Ÿçš„æµ®ç‚¹è¿ç®—ã€‚æ›´å¼ºå¤§çš„å¤„ç†å™¨æ„å‘³ç€å¾æœåŒæ ·çš„è®¡ç®—é¢†åŸŸéœ€è¦çš„èŠ¯ç‰‡æ›´å°‘ã€‚

## ğŸ“ˆ æ‰©å±•å›¾è¡¨ï¼šæ¨¡å‹åŠå…¶è®¡ç®—éœ€æ±‚
ä»BERTåˆ°GPT-3ï¼Œæ¯ç§æ¨¡å‹å˜ä½“ä»¥ä¸åŒçš„æ–¹å¼ä»è®¡ç®—äº•ä¸­é¥®æ°´ã€‚æœ€å¤§çš„GPT-3å˜ä½“ï¼Œæ‹¥æœ‰1750äº¿å‚æ•°ï¼ŒæƒŠäººåœ°æ¶ˆè€—äº†3700åƒä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®—ç§’/å¤©ã€‚

## ğŸ” å¹‚å¾‹æ‚–è®ºï¼šæ­ç¤ºæ€§èƒ½ç§˜å¯†
å¹‚å¾‹å…³ç³»å†³å®šäº†è®¡ç®—é¢„ç®—å’Œæ¨¡å‹æ€§èƒ½ä¹‹é—´çš„èˆè¹ˆã€‚æ›´å¤šçš„è®¡ç®—é€šå¸¸ä¼šå¯¼è‡´æ›´å¥½çš„æ€§èƒ½ï¼Œä½†ç¡¬ä»¶è®¿é—®å’Œæ—¶é—´ç­‰ç°å®ä¸–ç•Œçš„é™åˆ¶å¯èƒ½ä¼šé™åˆ¶ä½ çš„è®­ç»ƒé›„å¿ƒã€‚

### ğŸ”— è®­ç»ƒæ•°æ®é›†ä¸æ¨¡å‹å¤§å°çš„ååŒæ•ˆåº”
ç ”ç©¶äººå‘˜å‘ç°ï¼Œè®­ç»ƒæ•°æ®é›†å¤§å°ã€æ¨¡å‹å¤§å°å’Œæµ‹è¯•æŸå¤±ä¹‹é—´å­˜åœ¨å¹‚å¾‹å…³ç³»ã€‚æœ€ä½³ç‚¹ï¼Ÿä¸€ä¸ªæ¯”æ¨¡å‹å‚æ•°æ•°é‡å¤§çº¦20å€çš„è®­ç»ƒæ•°æ®é›†ã€‚

## ğŸ“ ã€Š Chinchillaè®ºæ–‡ã€‹ï¼šè®¡ç®—æœ€ä¼˜å®£è¨€
2022å¹´ï¼Œç”±Jordan Hoffmannã€Sebastian Borgeaudå’ŒArthur Mensché¢†å¯¼çš„ç ”ç©¶äººå‘˜é€šè¿‡Chinchillaæ¨¡å‹æ­ç¤ºäº†æœ€ä½³å¹³è¡¡ã€‚ä»–ä»¬æå‡ºï¼Œè®­ç»ƒæœ‰ç´ çš„è¾ƒå°æ¨¡å‹å¯ä»¥èƒœè¿‡é‚£äº›è®­ç»ƒä¸è¶³çš„è¾ƒå¤§æ¨¡å‹ã€‚

### ğŸ† Chinchillaçš„èƒœåˆ©
Chinchillaè¡¨æ˜ï¼Œåªè¦è®¡ç®—é¢„ç®—å’Œè®­ç»ƒæ•°æ®é›†å¤§å°å¾—å½“ï¼Œæ¨¡å‹å°±èƒ½åœ¨ä¸€ç³»åˆ—ä»»åŠ¡ä¸Šå®ç°æ¯”éè®¡ç®—æœ€ä¼˜çš„å·¨å…½ï¼ˆå¦‚GPT-3ï¼‰æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚

## ğŸ’¡ æ¨¡å‹è®¾è®¡çš„æœªæ¥ï¼šå°ä¸ä¸€å®šæ€»æ˜¯å°‘
å½“æˆ‘ä»¬çª¥è§†AIçš„æ°´æ™¶çƒæ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€ç§ä»â€œè¶Šå¤§è¶Šå¥½â€çš„å£å·ä¸­è½¬å˜çš„è¶‹åŠ¿ã€‚æ‹¥æœ‰500äº¿å‚æ•°çš„Bloomberg GPTï¼Œä»¥è®¡ç®—æœ€ä¼˜çš„æ–¹å¼è®­ç»ƒï¼Œä»¥ç²¾ç¡®æ‰§è¡Œä»»åŠ¡ã€‚

## ğŸ”® æ€»ç»“ï¼šLLMè®­ç»ƒçš„å‰è¿›é“è·¯
å€ŸåŠ©Chinchillaè®ºæ–‡çš„ç»éªŒæ•™è®­ï¼Œä½ ç°åœ¨å¯ä»¥è®­ç»ƒä¸ä»…æ›´å¤§ã€æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆï¼Œè€Œä¸”å®Œç¾å¹³è¡¡å…¶è®¡ç®—é¢„ç®—çš„æ¨¡å‹ã€‚

ä¸è¦å¿˜è®°è®¢é˜…ï¼Œæ·±å…¥äº†è§£AIæ·±æ¸Šã€‚æˆ‘ä»¬åœ¨è¿™é‡Œå¼•å¯¼ä½ ç©¿è¶Šæ¨¡å‹è®­ç»ƒçš„é™©æ¶åœ°å½¢ï¼

ğŸ‘‹ ä¸‹æ¬¡è§ï¼Œç»§ç»­è®­ç»ƒï¼Œç»§ç»­åˆ›æ–°ï¼Œæ„¿ä½ çš„æ¨¡å‹æ°¸è¿œè®¡ç®—æœ€ä¼˜ï¼

---

[åŠ å…¥æˆ‘ä»¬ï¼Œè·å–æ›´å¤šå…³äºLLMè®­ç»ƒåŠå…¶å®ƒçš„æ´è§ï¼](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---

# ğŸŒŸ The Great Model Balancing Act: Striking Gold with Compute Optimal LLMs

Hey Tech Adepts! ğŸ§™â€â™€ï¸ Join us as we weave through the computational enchantments of training Large Language Models (LLMs). Today, we're unlocking the secrets of model size, dataset breadth, and the mystical compute budget. Get ready to harness the power of optimal pre-training!

## ğŸ“Š The Power of Scale: Datasets and Parameters
In the quest for pre-training supremacy, you've got two mighty weapons: the size of your dataset and the number of parameters in your model. Theoretically, more is more, but beware the compute budget dragon!

### ğŸ§­ Navigating the Compute Budget Seas
A petaFLOP per second day is your sextant, charting the floating-point operations across your GPU fleet. More powerful processors mean fewer chips needed to conquer the same compute land.

## ğŸ“ˆ The Scaling Charts: Models and Their Computational Thirst
From BERT to GPT-3, each model variant drinks from the compute well differently. The largest GPT-3 variant, with its 175 billion parameters, guzzles a staggering 3,700 petaFLOP per second days.

## ğŸ” The Power-Law Paradox: Unveiling Performance Secrets
A power-law relationship dictates the dance between compute budget and model performance. More compute typically leads to better performance, but real-world constraints like hardware access and time can cap your training ambitions.

### ğŸ”— The Training Dataset and Model Size Synergy
Researchers have discovered a power-law relationship between training dataset size, model size, and test loss. The sweet spot? A training dataset about 20 times larger than the number of model parameters.

## ğŸ“ The Chinchilla Paper: The Compute Optimal Manifesto
In 2022, researchers led by Jordan Hoffmann, Sebastian Borgeaud, and Arthur Mensch revealed the optimal balance with the Chinchilla model. Smaller but well-trained models, they proposed, could outperform their larger, under-trained counterparts.

### ğŸ† The Chinchilla Triumph
Chinchilla showed that with the right compute budget and training dataset size, models can achieve superior performance on a range of tasks compared to non-compute optimal behemoths like GPT-3.

## ğŸ’¡ The Future of Model Design: Smaller Isn't Always Less
As we peer into the crystal ball of AI, we see a shift away from the "bigger is better" mantra. The Bloomberg GPT, with its 50 billion parameters, is a shining star, trained in a compute optimal way to perform tasks with precision.

## ğŸ”® Wrapping Up: The Path Forward for LLM Training
With the lessons of the Chinchilla paper, you're now equipped to train models that are not just bigger, but smarter, more efficient, and perfectly balanced for their compute budgets.

Don't forget to subscribe for more deep dives into the AI abyss. We're here to guide you through the treacherous terrain of model training!

ğŸ‘‹ Until next time, keep training, keep innovating, and may your models always be compute optimal!

---

[Join us for more insights on LLM training and beyond!](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch) 

---

# ç§‘æ™®æŠ€æœ¯æ–‡ç« ï¼šæ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„è®¡ç®—ä¼˜åŒ–

## å¼•è¨€
è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºã€‚æœ¬æ–‡å°†æ¢è®¨æ¨¡å‹å¤§å°ã€è®­ç»ƒé…ç½®å’Œæ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œä»¥åŠå¦‚ä½•åœ¨æœ‰é™çš„è®¡ç®—é¢„ç®—ä¸‹ä¼˜åŒ–æ¨¡å‹è®­ç»ƒã€‚

## è®¡ç®—é¢„ç®—ä¸æ€§èƒ½
åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–æ¨¡å‹åœ¨å­¦ä¹ ç›®æ ‡ä¸Šçš„æ€§èƒ½ï¼Œå³åœ¨é¢„æµ‹è¯å…ƒæ—¶æœ€å°åŒ–æŸå¤±ã€‚ç†è®ºä¸Šï¼Œå¯ä»¥é€šè¿‡å¢åŠ è®­ç»ƒæ•°æ®é›†çš„å¤§å°æˆ–æ¨¡å‹çš„å‚æ•°æ•°é‡æ¥æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œè®¡ç®—é¢„ç®—ï¼ˆåŒ…æ‹¬å¯ç”¨GPUæ•°é‡å’Œè®­ç»ƒæ—¶é—´ï¼‰æ˜¯ä¸€ä¸ªé‡è¦è€ƒè™‘å› ç´ ã€‚

## è®¡ç®—å•ä½ï¼špetaFLOP/ç§’/å¤©
petaFLOP/ç§’/å¤©æ˜¯è¡¡é‡è®¡ç®—èµ„æºçš„å•ä½ï¼Œè¡¨ç¤ºä»¥æ¯ç§’petaFLOPï¼ˆ10^15æ¬¡æµ®ç‚¹è¿ç®—ï¼‰çš„é€Ÿåº¦è¿è¡Œä¸€æ•´å¤©ã€‚ä¾‹å¦‚ï¼Œä¸¤ä¸ªNVIDIA A100 GPUæä¾›çš„è®¡ç®—èƒ½åŠ›ç›¸å½“äºå…«ä¸ªV100 GPUã€‚

## æ¨¡å‹å¤§å°ã€æ•°æ®é›†å¤§å°ä¸è®¡ç®—é¢„ç®—çš„å…³ç³»
ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹å¤§å°ã€è®­ç»ƒæ•°æ®é›†å¤§å°å’Œè®¡ç®—é¢„ç®—ä¹‹é—´å­˜åœ¨æ˜ç¡®çš„å…³ç³»ã€‚OpenAIçš„ç ”ç©¶äººå‘˜å‘ç°ï¼Œæ¨¡å‹æ€§èƒ½ï¼ˆä»¥æµ‹è¯•æŸå¤±è¡¨ç¤ºï¼‰ä¸è®¡ç®—é¢„ç®—ä¹‹é—´å­˜åœ¨å¹‚å¾‹å…³ç³»ã€‚

## è®­ç»ƒæ•°æ®é›†å¤§å°ä¸æ¨¡å‹å¤§å°çš„ä¼˜åŒ–
ç ”ç©¶è¿˜å‘ç°ï¼Œè®­ç»ƒæ•°æ®é›†å¤§å°å’Œæ¨¡å‹å¤§å°ä¸æµ‹è¯•æŸå¤±ä¹‹é—´ä¹Ÿå­˜åœ¨å¹‚å¾‹å…³ç³»ã€‚è¿™æ„å‘³ç€ï¼Œå¯¹äºç»™å®šçš„è®¡ç®—é¢„ç®—ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´æ•°æ®é›†å¤§å°å’Œæ¨¡å‹å¤§å°æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚

## Chinchillaæ¨¡å‹ï¼šè®¡ç®—ä¼˜åŒ–çš„æ¡ˆä¾‹
2022å¹´çš„ä¸€é¡¹ç ”ç©¶æå‡ºäº†Chinchillaæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªè®¡ç®—ä¼˜åŒ–çš„æ¨¡å‹ï¼Œå±•ç¤ºäº†åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å¦‚ä½•é€šè¿‡è°ƒæ•´æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ•°æ®é›†å¤§å°æ¥å®ç°æœ€ä½³æ€§èƒ½ã€‚

### Chinchillaæ¨¡å‹çš„å…³é”®å‘ç°
- æœ€ä½³è®­ç»ƒæ•°æ®é›†å¤§å°çº¦ä¸ºæ¨¡å‹å‚æ•°æ•°é‡çš„20å€ã€‚
- Chinchillaæ¨¡å‹ï¼ˆ70äº¿å‚æ•°ï¼‰çš„ç†æƒ³è®­ç»ƒæ•°æ®é›†åŒ…å«1.4ä¸‡äº¿è¯å…ƒã€‚
- è®¡ç®—éä¼˜åŒ–æ¨¡å‹ï¼ˆå¦‚GPT-3ï¼‰å¯èƒ½åœ¨æ›´å¹¿æ³›çš„ä¸‹æ¸¸è¯„ä¼°ä»»åŠ¡ä¸­è¡¨ç°ä¸å¦‚Chinchillaã€‚

## ç»“è¯­
éšç€å¯¹è®¡ç®—ä¼˜åŒ–æ¨¡å‹çš„è®¤è¯†ä¸æ–­æ·±å…¥ï¼Œæœªæ¥å¯èƒ½ä¼šçœ‹åˆ°å¯¹â€œè¶Šå¤§è¶Šå¥½â€è¶‹åŠ¿çš„åç¦»ï¼Œæ›´å¤šçš„ç ”ç©¶å›¢é˜Ÿå’Œå¼€å‘è€…å°†å¼€å§‹ä¼˜åŒ–ä»–ä»¬çš„æ¨¡å‹è®¾è®¡ã€‚Bloomberg GPTç­‰æ¨¡å‹å±•ç¤ºäº†åœ¨è®¡ç®—ä¼˜åŒ–è®­ç»ƒä¸‹ï¼Œè¾ƒå°çš„æ¨¡å‹ä¹Ÿèƒ½å®ç°ä¸å¤§å‹æ¨¡å‹ç›¸ä¼¼ç”šè‡³æ›´å¥½çš„ç»“æœã€‚

---

æœ¬æ–‡ä¸ºè¯»è€…æä¾›äº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­è®¡ç®—ä¼˜åŒ–é‡è¦æ€§çš„æ·±å…¥ç†è§£ï¼Œå¸®åŠ©ä»–ä»¬åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹åšå‡ºæ›´æ˜æ™ºçš„æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒç­–ç•¥é€‰æ‹©ã€‚

---

[åŠ å…¥æˆ‘ä»¬ï¼Œè·å–æ›´å¤šå…³äºLLMè®­ç»ƒåŠå…¶å®ƒçš„æ´è§ï¼](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---
