# ğŸš€ å®šåˆ¶åŒ–LLMsï¼šä½•æ—¶ä»å¤´å¼€å§‹è®­ç»ƒè‡ªå·±çš„æ¨¡å‹

å˜¿ï¼ŒæŠ€æœ¯åˆ›æ–°è€…ä»¬ï¼ğŸŒŸ åŠ å…¥æˆ‘ä»¬ï¼Œè¶…è¶Šå¹³å‡¡ï¼Œæ·±å…¥åˆ°ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯èƒ½ä¸è¶³çš„ä¸“ä¸šåŒ–é¢†åŸŸã€‚å‡†å¤‡å¥½æ¢ç´¢ä»å¤´å¼€å§‹é¢„è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ä¸ä»…æ˜¯é€‰æ‹©ï¼Œè€Œæ˜¯å¿…éœ€çš„åœºæ™¯ï¼

## ğŸŒ LLMä¸“ä¸šåŒ–çš„å›°å¢ƒ
åˆ©ç”¨ç°æœ‰çš„LLMså¯ä»¥å°†ä½ çš„åº”ç”¨æ¨å‘åŸå‹å¤©å ‚ï¼Œä½†å½“ä½ çš„é¢†åŸŸä½¿ç”¨ç‰¹å®šè¯­è¨€æ—¶ä¼šæœ‰é—®é¢˜â€”â€”æƒ³æƒ³æ³•å¾‹ã€åŒ»å­¦æˆ–é‡‘èã€‚

### ğŸ“š é¢†åŸŸç‰¹å®šè¯­è¨€çš„æŒ‘æˆ˜
åƒâ€œçŠ¯ç½ªæ„å›¾â€è¿™æ ·çš„æ³•å¾‹æœ¯è¯­æˆ–åŒ»å­¦é€Ÿè®°å¹¶ä¸æ˜¯æ—¥å¸¸èŠå¤©çš„å†…å®¹ï¼Œå¾ˆå¯èƒ½æ²¡æœ‰å‡ºç°åœ¨é€šç”¨LLMçš„è®­ç»ƒæ–‡æœ¬ä¸­ã€‚è¿™å¯èƒ½ä¼šè®©ä½ çš„æ¨¡å‹åœ¨æœ¯è¯­å’Œä¸Šä¸‹æ–‡ä¸­æŒ£æ‰ã€‚

### ğŸ” é¢†åŸŸé€‚åº”çš„å¿…è¦æ€§
å½“æ—¥å¸¸è¯æ±‡åœ¨ä½ çš„é¢†åŸŸä¸­å…·æœ‰ç‰¹æ®Šå«ä¹‰â€”â€”æ¯”å¦‚åˆåŒä¸­çš„â€œå¯¹ä»·â€â€”â€”ç°æœ‰çš„LLMså¯èƒ½éš¾ä»¥è·Ÿä¸Šï¼Œéœ€è¦ä¸€ä¸ªé’ˆå¯¹ä½ é¢†åŸŸè¯­è¨€æ™¯è§‚è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ã€‚

## ğŸ¥ BloombergGPTï¼šä¸€ä¸ªé¢†åŸŸç‰¹å®šLLMæ¡ˆä¾‹ç ”ç©¶
é‡è§BloombergGPTï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‘èé¢†åŸŸçš„LLMï¼Œé¢„è®­ç»ƒä»¥åœ¨é‡‘èåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸Šä¿æŒè‡ªå·±çš„ç‰¹è‰²ã€‚

### ğŸ¤– æœ‰ç›®çš„åœ°å®šåˆ¶æ¨¡å‹
é€šè¿‡å°†é‡‘èæ•°æ®ä¸ä¸€èˆ¬æ•°æ®é›†æ··åˆï¼Œå½­åšç¤¾çš„ç ”ç©¶äººå‘˜æ‰“é€ äº†ä¸€ä¸ªæ—¢ä¸“ä¸šåŒ–åˆå¹¿æ³›èƒœä»»çš„æ¨¡å‹ï¼Œå±•ç¤ºäº†é¢†åŸŸç‰¹å®šé¢„è®­ç»ƒçš„åŠ›é‡ã€‚

## ğŸ“ˆ è§„æ¨¡æ³•åˆ™å’Œæƒè¡¡
åœ¨å®è¯è§„æ¨¡æ³•åˆ™çš„æŒ‡å¯¼ä¸‹ï¼Œå‘ç°æ¨¡å‹å¤§å°ã€è®­ç»ƒæ•°æ®é›†ä½“ç§¯å’Œè®¡ç®—é¢„ç®—ä¹‹é—´çš„å¹³è¡¡è¡Œä¸ºã€‚

### ğŸ“‰ BloombergGPTçš„æ–¹æ³•
BloombergGPTçš„æ¨¡å‹å¤§å°ä¸Chinchillaè®ºæ–‡çš„è®¡ç®—æœ€ä¼˜è§„æ¨¡æ³•åˆ™éå¸¸å»åˆï¼Œè¡¨æ˜å¯¹äºç»™å®šé¢„ç®—æ¥è¯´å‚æ•°æ•°é‡æ¥è¿‘æœ€ä¼˜ã€‚

### ğŸ“Š è®­ç»ƒæ•°æ®é›†çš„ç°å®
ç”±äºé¢†åŸŸæ•°æ®é™åˆ¶ï¼Œè®­ç»ƒæ•°æ®é›†æ¯”Chinchillaçš„å»ºè®®å°ï¼ŒBloombergGPTè¡¨æ˜ç°å®ä¸–ç•Œçš„å› ç´ å¯èƒ½å½±å“ä½ çš„é¢„è®­ç»ƒå†³ç­–ã€‚

## ğŸ”¬ ç¬¬ä¸€å‘¨çš„å›é¡¾å’Œåæ€
ä½ å·²ç»ç»å†äº†LLMç”¨ä¾‹ã€å˜å‹å™¨æ¶æ„ã€æ¨ç†æ—¶å‚æ•°å’Œç”Ÿæˆå¼AIé¡¹ç›®ç”Ÿå‘½å‘¨æœŸçš„æ—…ç¨‹ã€‚æ­¤å¤–ï¼Œä½ å·²ç»è§£å†³äº†é¢„è®­ç»ƒæŒ‘æˆ˜å’Œè®¡ç®—æœ€ä¼˜æ¨¡å‹è®¾è®¡çš„è§„æ¨¡æ³•åˆ™ã€‚

## ğŸ”® æ€»ç»“ï¼šè§„åˆ’ä½ çš„LLMè¯¾ç¨‹
å½“ä½ ç«™åœ¨ä½ çš„LLMé¡¹ç›®çš„èˆµæ‰‹ä½ç½®æ—¶ï¼Œè®°ä½æœ‰æ—¶è¾ƒå°‘äººèµ°çš„è·¯â€”â€”æ¯”å¦‚é¢„è®­ç»ƒä½ è‡ªå·±çš„æ¨¡å‹â€”â€”æ˜¯é€šå¾€åˆ›æ–°å’Œé¢†åŸŸæŒæ¡çš„é“è·¯ã€‚

ä¸è¦å¿˜è®°è®¢é˜…ï¼Œæ·±å…¥äº†è§£AIå‰æ²¿ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œå¼•å¯¼ä½ ç©¿è¶ŠAIå¼€å‘çš„æœªçŸ¥é¢†åŸŸï¼

ğŸ‘‹ ä¸‹æ¬¡è§ï¼Œç»§ç»­æ¢ç´¢ï¼Œç»§ç»­é€‚åº”ï¼Œæ„¿ä½ çš„æ¨¡å‹æ°¸è¿œå®Œç¾åœ°è°ƒæ•´åˆ°é¢†åŸŸï¼

---

[åŠ å…¥æˆ‘ä»¬ï¼Œäº†è§£æ›´å¤šå…³äºLLMä¸“ä¸šåŒ–åŠå…¶å®ƒå†…å®¹ï¼](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---

# ğŸš€ Specializing LLMs: When to Train Your Own Model from Scratch

Hey Tech Innovators! ğŸŒŸ Join us as we venture beyond the ordinary and dive into the specialized realms where existing Large Language Models (LLMs) might fall short. Get ready to explore the scenarios where pretraining your own model from scratch is not just an option, but a necessity!

## ğŸŒ The LLM Specialization Quandary
While leveraging existing LLMs can propel your application to prototype paradise, there's a catch when your domain speaks a unique languageâ€”think law, medicine, or finance.

### ğŸ“š Domain-Specific Language Challenges
Legal jargon like "mens rea" or medical shorthand isn't everyday chitchat, and it's likely absent from the training texts of general LLMs. This can leave your model fumbling with terminology and context.

### ğŸ” The Need for Domain Adaptation
When everyday words take on special meanings in your domainâ€”like "consideration" in contractsâ€”existing LLMs may struggle to keep up, calling for a model fine-tuned to your domain's linguistic landscape.

## ğŸ¥ BloombergGPT: A Domain-Specific LLM Case Study
Meet BloombergGPT, a finance-savvy LLM pretrained to ace financial benchmarks while holding its own on general tasks.

### ğŸ¤– Tailoring a Model with Purpose
By blending financial data with general datasets, Bloomberg's researchers crafted a model that's both specialized and broadly competent, demonstrating the power of domain-specific pretraining.

## ğŸ“ˆ Scaling Laws and Trade-offs
Discover the balancing act between model size, training dataset volume, and compute budget, guided by empirical scaling laws.

### ğŸ“‰ BloombergGPT's Approach
BloombergGPT's model size aligns closely with the Chinchilla paper's compute-optimal scaling laws, suggesting a near-optimal parameter count for the given budget.

### ğŸ“Š Training Dataset Realities
With a training dataset smaller than the Chinchilla recommendation due to domain data constraints, BloombergGPT shows that real-world factors can influence your pretraining decisions.

## ğŸ”¬ Recap and Reflections on Week One
You've journeyed through LLM use cases, the transformer architecture, inference-time parameters, and the generative AI project lifecycle. Plus, you've tackled pretraining challenges and scaling laws for compute-optimal model design.

## ğŸ”® Wrapping Up: Charting Your LLM Course
As you stand at the helm of your LLM project, remember that sometimes, the path less traveledâ€”like pretraining your own modelâ€”is the path that leads to innovation and domain mastery.

Don't forget to subscribe for more expeditions into the AI frontier. We're here to guide you through the uncharted territories of AI development!

ğŸ‘‹ Until next time, keep exploring, keep adapting, and may your models always be perfectly tuned to the domain!

---

[Join us for more on LLM specialization and beyond!](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---

# ç§‘æ™®æŠ€æœ¯æ–‡ç« ï¼šä¸ºä½•éœ€è¦ä»å¤´å¼€å§‹é¢„è®­ç»ƒä¸“ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹

## å¼•è¨€
è™½ç„¶é€šå¸¸å»ºè®®ä½¿ç”¨ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥èŠ‚çœæ—¶é—´å’Œèµ„æºï¼Œä½†åœ¨æŸäº›ç‰¹å®šæƒ…å†µä¸‹ï¼Œä»å¤´å¼€å§‹é¢„è®­ç»ƒè‡ªå·±çš„æ¨¡å‹å¯èƒ½æ˜¯å¿…è¦çš„ã€‚æœ¬æ–‡å°†æ¢è®¨è¿™ä¸€å†³å®šèƒŒåçš„åŠ¨æœºå’Œå®ä¾‹ã€‚

## ç‰¹å®šé¢†åŸŸçš„éœ€æ±‚
å½“ç›®æ ‡é¢†åŸŸä½¿ç”¨æ—¥å¸¸è¯­è¨€ä¸­ä¸å¸¸è§çš„è¯æ±‡å’Œè¯­è¨€ç»“æ„æ—¶ï¼Œå¦‚æ³•å¾‹ã€åŒ»å­¦ã€é‡‘èæˆ–ç§‘å­¦é¢†åŸŸï¼Œå¯èƒ½éœ€è¦è¿›è¡Œé¢†åŸŸé€‚åº”ä»¥å®ç°è‰¯å¥½çš„æ¨¡å‹æ€§èƒ½ã€‚

## é¢†åŸŸç‰¹å®šè¯æ±‡çš„æŒ‘æˆ˜
é¢†åŸŸç‰¹å®šè¯æ±‡å¦‚æ³•å¾‹æœ¯è¯­â€œmens reaâ€æˆ–åŒ»å­¦æœ¯è¯­å¯èƒ½åœ¨ç°æœ‰LLMsçš„è®­ç»ƒæ–‡æœ¬ä¸­å‡ºç°é¢‘ç‡ä¸é«˜ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥æ­£ç¡®ç†è§£æˆ–ä½¿ç”¨è¿™äº›æœ¯è¯­ã€‚

## è¯­è¨€ç»“æ„çš„ç‰¹æ®Šç”¨æ³•
å³ä½¿åœ¨é¢†åŸŸå†…ä½¿ç”¨æ—¥å¸¸è¯æ±‡ï¼Œå¦‚â€œconsiderationâ€åœ¨åˆåŒæ³•ä¸­çš„å«ä¹‰ï¼Œä¹Ÿå¯èƒ½ä¸é€šç”¨å«ä¹‰ä¸åŒï¼Œè¿™ç»™ç°æœ‰LLMsçš„åº”ç”¨å¸¦æ¥æŒ‘æˆ˜ã€‚

## ä»å¤´å¼€å§‹é¢„è®­ç»ƒçš„ä¼˜åŠ¿
ä»å¤´å¼€å§‹é¢„è®­ç»ƒçš„æ¨¡å‹å¯ä»¥æ›´å¥½åœ°é€‚åº”é«˜åº¦ä¸“ä¸šåŒ–çš„é¢†åŸŸï¼Œå› ä¸ºå®ƒä»¬é€šè¿‡åŸå§‹é¢„è®­ç»ƒä»»åŠ¡å­¦ä¹ è¯æ±‡å’Œè¯­è¨€ç†è§£ã€‚

## BloombergGPTæ¡ˆä¾‹ç ”ç©¶
BloombergGPTæ˜¯ä¸€ä¸ªä¸ºé‡‘èé¢†åŸŸé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†é‡‘èæ•°æ®å’Œé€šç”¨æ•°æ®ï¼Œä»¥åœ¨é‡‘èåŸºå‡†æµ‹è¯•ä¸­å–å¾—ä¼˜å¼‚ç»“æœï¼ŒåŒæ—¶ä¿æŒåœ¨é€šç”¨LLMåŸºå‡†æµ‹è¯•ä¸­çš„ç«äº‰åŠ›ã€‚

## æ¨¡å‹æ¶æ„ä¸è®­ç»ƒ
Bloombergç ”ç©¶äººå‘˜éµå¾ªChinchillaçš„ç¼©æ”¾æ³•åˆ™ï¼Œè®¨è®ºäº†æ¨¡å‹æ¶æ„å’Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¿…é¡»åšå‡ºçš„æƒè¡¡ã€‚

## è®¡ç®—é¢„ç®—ä¸æ¨¡å‹å¤§å°
BloombergGPTçš„æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ•°æ®é›†å¤§å°ä¸å…¶å¯ç”¨çš„è®¡ç®—é¢„ç®—ç›¸åŒ¹é…ï¼Œå±•ç¤ºäº†å¦‚ä½•åœ¨æœ‰é™èµ„æºä¸‹ä¼˜åŒ–æ¨¡å‹è®¾è®¡ã€‚

## ç»“è¯­
æœ¬å‘¨çš„å­¦ä¹ æ¶µç›–äº†LLMsçš„å¤šç§ç”¨é€”ã€å˜å‹å™¨æ¶æ„ã€æ¨ç†æ—¶çš„å‚æ•°å½±å“ã€ç”Ÿæˆå¼AIé¡¹ç›®ç”Ÿå‘½å‘¨æœŸã€æ¨¡å‹é¢„è®­ç»ƒçš„è®¡ç®—æŒ‘æˆ˜ã€é‡åŒ–æŠ€æœ¯ï¼Œä»¥åŠLLMsçš„ç¼©æ”¾æ³•åˆ™ã€‚é€šè¿‡è¿™äº›çŸ¥è¯†ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ˜æ™ºåœ°å†³å®šä½•æ—¶ä½¿ç”¨ç°æœ‰æ¨¡å‹ï¼Œä½•æ—¶éœ€è¦ä»å¤´å¼€å§‹é¢„è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ã€‚

---

æœ¬æ–‡ä¸ºè¯»è€…æä¾›äº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„æ·±å…¥ç†è§£ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹ç‰¹å®šé¢†åŸŸæŒ‘æˆ˜æ—¶å¦‚ä½•åšå‡ºå†³ç­–ï¼Œä»¥åŠå¦‚ä½•æ ¹æ®è®¡ç®—é¢„ç®—å’Œé¢†åŸŸéœ€æ±‚è®¾è®¡æœ€ä¼˜æ¨¡å‹ã€‚

---

[åŠ å…¥æˆ‘ä»¬ï¼Œäº†è§£æ›´å¤šå…³äºLLMä¸“ä¸šåŒ–åŠå…¶å®ƒå†…å®¹ï¼](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---
