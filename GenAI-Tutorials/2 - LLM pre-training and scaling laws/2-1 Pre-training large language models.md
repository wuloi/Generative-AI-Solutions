# ğŸš€ å¯¼èˆªLLMé¢†åŸŸï¼šä¸ºä½ çš„AIå¥¥å¾·èµ›é€‰æ‹©æ­£ç¡®çš„æ¨¡å‹

å˜¿ï¼ŒæŠ€æœ¯å…ˆé”‹ä»¬ï¼ğŸ‘‹ å‡†å¤‡å¥½è¸ä¸Šä½ çš„ç”Ÿæˆå¼AIä¹‹æ—…äº†å—ï¼Ÿä»Šå¤©ï¼Œæˆ‘ä»¬å°†è§„åˆ’å¦‚ä½•ä¸ºä½ çš„åº”ç”¨é€‰æ‹©å®Œç¾çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æ˜¯æ—¶å€™æ·±å…¥é¢„è®­ç»ƒã€æ¨¡å‹æ¶æ„å’ŒLLMçš„ç»†å¾®å·®åˆ«çš„ä¸–ç•Œäº†ï¼

## ğŸŒˆ ç”Ÿæˆå¼AIé¡¹ç›®ç”Ÿå‘½å‘¨æœŸï¼šä»æ¦‚å¿µåˆ°åˆ›é€ 
åœ¨æˆ‘ä»¬ä¸Šæ¬¡çš„å†’é™©ä¸­ï¼Œæˆ‘ä»¬ä¸ºä½ çš„AIé¡¹ç›®åˆ¶å®šäº†è·¯çº¿å›¾ã€‚ç°åœ¨ï¼Œæ˜¯æ—¶å€™å¼€å§‹æˆ‘ä»¬æ‰‹å¤´çš„ç¬¬ä¸€ä¸ªå…³é”®æ­¥éª¤ï¼šé€‰æ‹©ä¸€ä¸ªæ¨¡å‹ã€‚

### ğŸ èµ·è·‘çº¿ï¼šé€‰æ‹©æˆ–è®­ç»ƒï¼Ÿ
ä½ æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ï¼Œè¿˜æ˜¯åˆ©ç”¨ç°æœ‰çš„æ¨¡å‹ï¼Ÿé€‰æ‹©å–å†³äºä½ çš„ç”¨ä¾‹ï¼Œä½†å¯¹å¤§å¤šæ•°äººæ¥è¯´ï¼Œç°æœ‰çš„å¼€æºæ¨¡å‹æ˜¯æœ€ä½³é€‰æ‹©ã€‚

### ğŸ“š æ¨¡å‹ä¸­å¿ƒï¼šä½ çš„LLMåº“
æ·±å…¥AIæ¡†æ¶å¼€å‘è€…å¦‚Hugging Faceå’ŒPyTorchç­–åˆ’çš„ä¸­å¿ƒã€‚è¿™äº›å®åº“å……æ»¡äº†æ¨¡å‹åŠå…¶â€œæ¨¡å‹å¡â€ï¼Œè¯¦ç»†è¯´æ˜äº†ç”¨ä¾‹ã€è®­ç»ƒå’Œé™åˆ¶ã€‚

## ğŸ› ï¸ LLMè®­ç»ƒçš„è§£å‰–å­¦ï¼šé¢„è®­ç»ƒè§£å¯†
LLMså°±åƒæµ·ç»µï¼Œåœ¨é¢„è®­ç»ƒé˜¶æ®µå¸æ”¶å¤§é‡çš„éç»“æ„åŒ–æ–‡æœ¬ã€‚è¿™å°±æ˜¯å®ƒä»¬å‘å±•å¯¹è¯­è¨€æ¨¡å¼å’Œç»“æ„çš„æ·±åˆ»ç†è§£çš„åœ°æ–¹ã€‚

### ğŸŒŸ é¢„è®­ç»ƒçš„åŠ›é‡ï¼šè‡ªç›‘ç£å­¦ä¹ 
ä»åƒå…†å­—èŠ‚åˆ°æ‹å­—èŠ‚ï¼ŒLLMså¤„ç†æ¥è‡ªäº’è”ç½‘å’Œç­–åˆ’è¯­æ–™åº“çš„æ–‡æœ¬ã€‚è¿™ä¸ªè‡ªç›‘ç£å­¦ä¹ é˜¶æ®µå¡‘é€ äº†æ¨¡å‹çš„è¯­è¨€ç†è§£ã€‚

### ğŸ”§ æ•°æ®è´¨é‡å’Œç­–åˆ’ï¼šè¿‡æ»¤è¿‡ç¨‹
ä»ç½‘ç»œæŠ“å–æ•°æ®ï¼Ÿé¢„è®¡è¦å¤„ç†å’Œè¿‡æ»¤ä»¥æé«˜è´¨é‡å’Œæ¶ˆé™¤åè§ã€‚åªæœ‰æœ€å¥½çš„1-3%çš„æ ‡è®°ä¼šè¿›å…¥é¢„è®­ç»ƒã€‚

## ğŸ›ï¸ LLMæ¶æ„çš„æ”¯æŸ±ï¼šç¼–ç å™¨ã€è§£ç å™¨å’Œåºåˆ—åˆ°åºåˆ—
è®©æˆ‘ä»¬åˆ†è§£ä¸‰ç§ä¸»è¦çš„å˜å‹å™¨æ¨¡å‹æ¶æ„åŠå…¶é¢„è®­ç»ƒç›®æ ‡ï¼š

### ğŸ­ ä»…ç¼–ç å™¨æ¨¡å‹ï¼šåŒå‘é‡å…½
- ä¹Ÿç§°ä¸ºè‡ªåŠ¨ç¼–ç æ¨¡å‹ã€‚
- ä½¿ç”¨æ©è”½è¯­è¨€å»ºæ¨¡è¿›è¡Œé¢„è®­ç»ƒã€‚
- é€‚ç”¨äºå¥å­åˆ†ç±»å’Œæ ‡è®°çº§ä»»åŠ¡ã€‚

### ğŸ“¤ ä»…è§£ç å™¨æ¨¡å‹ï¼šå•å‘å¼€æ‹“è€…
- ä¹Ÿç§°ä¸ºè‡ªå›å½’æ¨¡å‹ã€‚
- ä½¿ç”¨å› æœè¯­è¨€å»ºæ¨¡è¿›è¡Œé¢„è®­ç»ƒã€‚
- ä¸ºæ–‡æœ¬ç”Ÿæˆå’Œå¼ºå¤§çš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›è€Œæ„å»ºã€‚

### ğŸ”„ åºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼šåŒå‘å·¨å¤´
- åŒæ—¶ä½¿ç”¨ç¼–ç å™¨å’Œè§£ç å™¨ç»„ä»¶ã€‚
- ä½¿ç”¨è·¨åº¦è…è´¥ç­‰ç›®æ ‡è¿›è¡Œè®­ç»ƒã€‚
- é€‚ç”¨äºç¿»è¯‘ã€æ‘˜è¦å’Œé—®ç­”ç­‰å¤šæ ·åŒ–ä»»åŠ¡ã€‚

## ğŸ“Š æ¨¡å‹å¤§å°å¾ˆé‡è¦ï¼šè¶Šå¤§ï¼Œèƒ½åŠ›è¶Šå¼º
è¾ƒå¤§çš„æ¨¡å‹å€¾å‘äºåœ¨ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ï¼Œæ¨åŠ¨äº†æ—¥ç›Šåºå¤§çš„LLMçš„å‘å±•ã€‚ä½†è¦æ³¨æ„ï¼Œå¹¶éæ€»æ˜¯è¶Šå¤§è¶Šå¥½â€”â€”è®­ç»ƒå·¨å¤§çš„æ¨¡å‹æ˜¯ä¸€é¡¹ä»£ä»·é«˜æ˜‚çš„å·¥ä½œã€‚

## ğŸ”® æ€»ç»“ï¼šè§„åˆ’ä½ çš„LLMè¯¾ç¨‹
æœ‰äº†è¿™äº›çŸ¥è¯†ï¼Œä½ å°±èƒ½é€‰æ‹©æœ€é€‚åˆä½ ç”¨ä¾‹çš„æ¨¡å‹ã€‚å½“ä½ è¿›å…¥LLMçš„ä¸–ç•Œæ—¶ï¼Œè¦å…³æ³¨æ¨¡å‹å¤§å°å’Œèƒ½åŠ›ä¹‹é—´çš„å¹³è¡¡ã€‚

åˆ«å¿˜äº†ç‚¹å‡»è®¢é˜…æŒ‰é’®ï¼Œè·å–æ›´å¤šè¿›å…¥AIå‰æ²¿çš„æ—…ç¨‹ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œå¼•å¯¼ä½ ç©¿è¶Šç”Ÿæˆå¼AIçš„å¤æ‚æ€§ï¼

ğŸ‘‹ ä¸‹æ¬¡è§ï¼Œç»§ç»­æ¢ç´¢ï¼Œç»§ç»­åˆ›æ–°ï¼Œæ„¿ä½ çš„æ¨¡å‹é€‰æ‹©æ€»æ˜¯æ°åˆ°å¥½å¤„ï¼

---

[åŠ å…¥æˆ‘ä»¬ï¼Œæ¢ç´¢æ›´å¤šAIï¼](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---

# ğŸš€ Navigating the LLM Landscape: Choosing the Right Model for Your AI Odyssey

Hey Tech Pioneers! ğŸ‘‹ Ready to embark on your generative AI journey? Today, we're charting the course to select the perfect Large Language Model (LLM) for your app. It's time to dive into the world of pre-training, model architectures, and the nuances of LLMs!

## ğŸŒˆ The Generative AI Project Life Cycle: From Concept to Creation
In our last adventure, we laid down the roadmap for your AI project. Now, it's time to get our hands dirty with the first crucial step: selecting a model.

### ğŸ The Starting Line: Choose or Train?
Do you train your own model from scratch or leverage an existing one? The choice depends on your use case, but for most, existing open-source models are the way to go.

### ğŸ“š Model Hubs: Your LLM Library
Jump into the curated hubs by AI framework developers like Hugging Face and PyTorch. These treasure troves are filled with models and their 'model cards' that detail use cases, training, and limitations.

## ğŸ› ï¸ The Anatomy of LLM Training: Pre-Training Demystified
LLMs are like sponges, absorbing vast amounts of unstructured text during their pre-training phase. This is where they develop a deep understanding of language patterns and structures.

### ğŸŒŸ The Power of Pre-Training: Self-Supervised Learning
From gigabytes to petabytes, LLMs process text from the Internet and curated corpora. This self-supervised learning phase shapes the model's language understanding.

### ğŸ”§ Data Quality and Curation: The Filtering Process
Scraping data from the web? Expect to process and filter to improve quality and remove bias. Only the best 1-3% of tokens make it to pre-training.

## ğŸ›ï¸ The Pillars of LLM Architecture: Encoder, Decoder, and Sequence-to-Sequence
Let's break down the three main types of transformer model architectures and their pre-training objectives:

### ğŸ­ Encoder-Only Models: The Bi-Directional Beasts
- Also known as autoencoding models.
- Pre-trained using masked language modeling.
- Ideal for sentence classification and token-level tasks.

### ğŸ“¤ Decoder-Only Models: The Unidirectional Trailblazers
- Also known as autoregressive models.
- Pre-trained using causal language modeling.
- Built for text generation and strong zero-shot inference abilities.

### ğŸ”„ Sequence-to-Sequence Models: The Two-Way Titans
- Utilize both encoder and decoder components.
- Trained using objectives like span corruption.
- Versatile for translation, summarization, and question-answering.

## ğŸ“Š Model Size Matters: The Larger, the More Capable
Larger models tend to perform better across tasks, driving the development of increasingly massive LLMs. But beware, bigger isn't always betterâ€”training huge models is a costly endeavor.

## ğŸ”® Wrapping Up: Charting Your LLM Course
With this knowledge, you're equipped to select the model that best fits your use case. As you venture into the world of LLMs, keep an eye on the balance between model size and capability.

Don't forget to hit that subscribe button for more journeys into the AI frontier. We're here to guide you through the complexities of generative AI!

ğŸ‘‹ Until next time, keep exploring, keep innovating, and may your model selections always be on point!

---

[Join us for more AI explorations!](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---

# ç§‘æ™®æŠ€æœ¯æ–‡ç« ï¼šå¦‚ä½•é€‰æ‹©é€‚åˆæ‚¨åº”ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹

## å¼•è¨€
åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åº”ç”¨çš„å¼€å‘è¿‡ç¨‹ä¸­ï¼Œé€‰æ‹©åˆé€‚çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯è‡³å…³é‡è¦çš„ä¸€æ­¥ã€‚æœ¬æ–‡å°†æŒ‡å¯¼æ‚¨å¦‚ä½•åœ¨ç°æœ‰çš„æ¨¡å‹åº“ä¸­é€‰æ‹©æˆ–è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ï¼Œå¹¶ä»‹ç»äº†ä¸åŒæ¨¡å‹æ¶æ„çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯ã€‚

## é€‰æ‹©ç°æœ‰æ¨¡å‹æˆ–ä»å¤´å¼€å§‹
åœ¨å¼€å‘åº”ç”¨æ—¶ï¼Œæ‚¨å¯ä»¥é€‰æ‹©ä½¿ç”¨ç°æœ‰çš„å¼€æºæ¨¡å‹ï¼Œæˆ–è€…æ ¹æ®è‡ªå·±çš„éœ€æ±‚ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªæ–°æ¨¡å‹ã€‚è®¸å¤šAIæ¡†æ¶ï¼Œå¦‚Hugging Faceå’ŒPyTorchï¼Œæä¾›äº†æ¨¡å‹åº“å’Œæ¨¡å‹å¡ç‰‡ï¼Œå¸®åŠ©å¼€å‘è€…äº†è§£æ¯ä¸ªæ¨¡å‹çš„æœ€ä½³ç”¨ä¾‹ã€è®­ç»ƒæ–¹å¼å’Œå±€é™æ€§ã€‚

## å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹
å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒé˜¶æ®µè·å¾—å¯¹è¯­è¨€çš„æ·±åˆ»ç†è§£ã€‚è¿™ä¸€é˜¶æ®µï¼Œæ¨¡å‹é€šè¿‡å¤§é‡éç»“æ„åŒ–æ–‡æœ¬æ•°æ®å­¦ä¹ è¯­è¨€çš„æ¨¡å¼å’Œç»“æ„ï¼Œè¿™äº›æ•°æ®å¯èƒ½æ¥è‡ªäº’è”ç½‘æˆ–ä¸“é—¨ä¸ºè®­ç»ƒè¯­è¨€æ¨¡å‹è€Œæ±‡ç¼–çš„æ–‡æœ¬åº“ã€‚

## æ¨¡å‹æ¶æ„å’Œè®­ç»ƒç›®æ ‡
- **ç¼–ç å™¨æ¨¡å‹**ï¼ˆå¦‚BERTå’ŒRoBERTaï¼‰ï¼šé€šè¿‡é®è”½è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œé€‚åˆéœ€è¦åŒå‘ä¸Šä¸‹æ–‡ç†è§£çš„ä»»åŠ¡ï¼Œå¦‚å¥å­åˆ†ç±»æˆ–å‘½åå®ä½“è¯†åˆ«ã€‚
- **è§£ç å™¨æ¨¡å‹**ï¼ˆå¦‚GPTå’ŒBLOOMï¼‰ï¼šé€šè¿‡å› æœè¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œé€‚åˆæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œä¹Ÿå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›ã€‚
- **åºåˆ—åˆ°åºåˆ—æ¨¡å‹**ï¼ˆå¦‚T5å’ŒBARTï¼‰ï¼šç»“åˆç¼–ç å™¨å’Œè§£ç å™¨ï¼Œé€šè¿‡ç‰¹å®šçš„é¢„è®­ç»ƒç›®æ ‡å­¦ä¹ ï¼Œé€‚åˆç¿»è¯‘ã€æ‘˜è¦å’Œé—®ç­”ç­‰ä»»åŠ¡ã€‚

## æ¨¡å‹é€‰æ‹©çš„è€ƒè™‘å› ç´ 
é€‰æ‹©æ¨¡å‹æ—¶ï¼Œè€ƒè™‘ä»»åŠ¡çš„å…·ä½“éœ€æ±‚å’Œæ¨¡å‹æ¶æ„çš„ç‰¹ç‚¹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨çš„åº”ç”¨éœ€è¦æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œè§£ç å™¨æ¨¡å‹æˆ–åºåˆ—åˆ°åºåˆ—æ¨¡å‹å¯èƒ½æ›´åˆé€‚ã€‚

## æ¨¡å‹å¤§å°ä¸æ€§èƒ½
é€šå¸¸ï¼Œæ›´å¤§çš„æ¨¡å‹åœ¨æ‰§è¡Œä»»åŠ¡æ—¶æ›´ä¸ºæœ‰æ•ˆï¼Œå› ä¸ºå®ƒä»¬æ‹¥æœ‰æ›´å¤šçš„å‚æ•°æ¥æ•æ‰è¯­è¨€çš„ç»†å¾®å·®åˆ«ã€‚ç„¶è€Œï¼Œè®­ç»ƒå¤§å‹æ¨¡å‹çš„æˆæœ¬å’Œéš¾åº¦ä¹Ÿåœ¨å¢åŠ ã€‚

## ç»“è¯­
é€šè¿‡æœ¬æ–‡ï¼Œæ‚¨åº”è¯¥å¯¹å¦‚ä½•é€‰æ‹©é€‚åˆæ‚¨åº”ç”¨åœºæ™¯çš„å¤§å‹è¯­è¨€æ¨¡å‹æœ‰äº†æ›´æ·±çš„ç†è§£ã€‚è®°ä½ï¼Œé€‰æ‹©æ¨¡å‹ä¸ä»…è¦è€ƒè™‘å…¶æ¶æ„å’Œè®­ç»ƒæ–¹å¼ï¼Œè¿˜è¦è€ƒè™‘æ¨¡å‹çš„å¤§å°å’Œæ‚¨çš„è®¡ç®—èµ„æºã€‚

---

æœ¬æ–‡ä¸ºè¯»è€…æä¾›äº†ä¸€ä¸ªæ¸…æ™°çš„æ¡†æ¶ï¼Œå¸®åŠ©ä»–ä»¬åœ¨ç”Ÿæˆå¼AIé¡¹ç›®ä¸­åšå‡ºå…³äºæ¨¡å‹é€‰æ‹©çš„å…³é”®å†³ç­–ï¼Œå¹¶ä»‹ç»äº†ä¸åŒæ¨¡å‹æ¶æ„å¦‚ä½•å½±å“æ¨¡å‹çš„é€‚ç”¨æ€§å’Œæ€§èƒ½ã€‚

---

[åŠ å…¥æˆ‘ä»¬ï¼Œæ¢ç´¢æ›´å¤šAIï¼](https://roadmaps.feishu.cn/wiki/RykrwFxPiiU4T7kZ63bc7Lqdnch)

---
